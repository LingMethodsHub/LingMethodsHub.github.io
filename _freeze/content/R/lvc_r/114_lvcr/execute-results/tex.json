{
  "hash": "d7166b647ac549c2964246599f4e93a5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Mixed-Efects Logistic Regression Analysis: Part 3\"\ndate: \"2026-01-19\"\nlicense: \"CC-BY-SA 4.0\"\ndescription: \"Doing a mixed-effects logistic regression analysis suitable for comparing to a *Goldvarb* analysis. Part 3: Correlations, Interactions, & Collinearity\"\ncrossref:\n  fig-title: Table  \n  fig-prefix: Table\n  fig-labels: arabic\nbibliography: references.bib\n---\n\n\n\n\n\nBefore you proceed with this section, please make sure that you have your data loaded and modified based on the code [here](https://lingmethodshub.github.io/content/R/lvc_r/050_lvcr.html) and that `Dep.Var` is [re-coded such that `Deletion` is the second factor](https://lingmethodshub.github.io/content/R/lvc_r/110_lvcr.html). Next, you [set the global *R* options to employ sum contrast coding](https://lingmethodshub.github.io/content/R/lvc_r/112_lvcr.html).\n\n\n## Correlations, Interactions, & Collinearity\n\nLets look again at the results of the most parsimonious analysis of the full data set. \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\ntd.glmer.parsimonious <-glmer(Dep.Var ~ After.New + Morph.Type + Before + Stress + Phoneme + (1|Speaker), data = td, family = \"binomial\", \n                              control = glmerControl(optCtrl = list(maxfun = 2e4), optimizer = \"bobyqa\"))\nsummary(td.glmer.parsimonious)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Dep.Var ~ After.New + Morph.Type + Before + Stress + Phoneme +  \n    (1 | Speaker)\n   Data: td\nControl: glmerControl(optCtrl = list(maxfun = 20000), optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n     1114      1175      -545      1090      1177 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-5.223 -0.488 -0.259  0.495 14.033 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n Speaker (Intercept) 0.796    0.892   \nNumber of obs: 1189, groups:  Speaker, 66\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -0.277      0.207   -1.34  0.18034    \nAfter.New1     1.840      0.157   11.71  < 2e-16 ***\nAfter.New2    -1.175      0.144   -8.14  4.1e-16 ***\nMorph.Type1    0.426      0.140    3.05  0.00230 ** \nMorph.Type2   -1.892      0.213   -8.87  < 2e-16 ***\nBefore1       -0.575      0.202   -2.84  0.00447 ** \nBefore2        0.526      0.193    2.72  0.00659 ** \nBefore3        0.117      0.278    0.42  0.67370    \nBefore4        0.731      0.190    3.85  0.00012 ***\nStress1       -0.799      0.137   -5.81  6.2e-09 ***\nPhoneme1       0.287      0.128    2.25  0.02462 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Aft.N1 Aft.N2 Mrp.T1 Mrp.T2 Befor1 Befor2 Befor3 Befor4\nAfter.New1   0.064                                                        \nAfter.New2  -0.104 -0.430                                                 \nMorph.Type1 -0.434  0.203 -0.114                                          \nMorph.Type2 -0.051 -0.221  0.178 -0.376                                   \nBefore1     -0.296 -0.223  0.293  0.052  0.429                            \nBefore2     -0.164  0.191 -0.094 -0.110  0.247  0.029                     \nBefore3      0.150  0.018 -0.060  0.319 -0.515 -0.421 -0.477              \nBefore4      0.250  0.304 -0.431 -0.202  0.051 -0.311 -0.090 -0.274       \nStress1     -0.434 -0.432 -0.064  0.050  0.097  0.056  0.125 -0.094 -0.250\nPhoneme1     0.459  0.149 -0.307 -0.137 -0.265 -0.543 -0.263  0.149  0.438\n            Strss1\nAfter.New1        \nAfter.New2        \nMorph.Type1       \nMorph.Type2       \nBefore1           \nBefore2           \nBefore3           \nBefore4           \nStress1           \nPhoneme1    -0.107\n```\n\n\n:::\n:::\n\nBelow the results for fixed effects is a table of the correlations of the fixed effects.  This table is a good way to spot non-[orthogonal](https://en.wikipedia.org/wiki/Orthogonality#Statistics,_econometrics,_and_economics) effects you might not yet have caught (though you should have caught these effects if you thoroughly explored your data using [summary statistics](https://lingmethodshub.github.io/content/R/lvc_r/060_lvcr.html)). Look at only the coefficients for the  correlations of levels of **different** parameters. Generally any value over $|0.3|$[^1] should be investigated further. If you have any correlations over $|0.7|$ you should be worried. In your table there is no correlation higher than $|0.7|$, but there are a few over $|0.3|$: `After.New1*Before4` $|0.304|$; `After.New1*Stress1` $|0.432|$; `After.New2*Before4` $|0.431|$; `After.New2*Phoneme1` $|0.307|$; `Morph.Type1*Before3` $|0.319|$; `Morph.Type2*Before3` $|0.515|$; `Before1*Phoneme1` $|0.543|$; and `Before4*Phoneme1` $|0.438|$. These correlations suggest  it might be worthwhile to re-check the summary statistics, looking especially at the cross-tab of `After.New` and `Before`, `After.New` and `Phoneme`, `Morph.Type` and `Before`, `Morph.Type` and `Phoneme`, and `Before` and `Phoneme`.[^2] \n\n[^2]: See also *Notes on Interactions* by Derek Denis, available at [https://www.dropbox.com/s/7c4tzc8st5dmeit/Denis_2010_Notes_On_Interactions.pdf](https://www.dropbox.com/s/7c4tzc8st5dmeit/Denis_2010_Notes_On_Interactions.pdf).\n[^1]: Any absolute value greater than $3$, or rather any positive value higher than $+3$ or any negative value lower than $-3$.\n\nThere are two other methods for testing for a relationship between your fixed effect predictors that relate to the kind of relationship your fixed effects predictors might have. The first is that the predictors have an **interaction** the other is that they are (multi-) **collinear**.\n\n### Interactions\nAn interaction arises when two independent fixed effects work together to predict the variation of the application value. For example, based on the [Conditional Inference Tree](https://lingmethodshub.github.io/content/R/lvc_r/080_lvcr.html) analysis you know that it is not case that gender itself explains the social variation in `Deletion` vs. `Realized` (this is confirmed by the [analysis of data from just young speakers](https://lingmethodshub.github.io/content/R/lvc_r/112_lvcr.html), where `Sex` is not significant), nor is it that age explains the social variation (confirmed by the non-significance of both `Age.Group` and `Center.Age` in [the full model](https://lingmethodshub.github.io/content/R/lvc_r/112_lvcr.html)). Instead, it seems that older men use `Deletion` more frequently than everyone else. This is an interaction. It is the combination of `Age.Group` and `Sex` that (potentially) best explains the social variation. You can test this in your model by creating an interaction group with these two fixed effect predictors. You do this by including the interaction term `Sex*Age.Group` in your analysis. To make things easier here you can simplify and again consider middle-age and older speakers together. You can also drop `Phoneme` as it is non-significant among both cohorts. When you include an interaction term, the individual components of the interaction will also be included as singular predictors. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a simplified Age.Group.Simple column\ntd <- td %>% mutate(Age.Group.Simple = cut(YOB, breaks = c(-Inf, 1979, Inf), \n                                  labels = c(\"Old/Middle\", \"Young\")))\n\n# Create a regression analysis with a Sex*Age.Group.Simple interaction group\ntd.glmer.sex.age.interaction <- glmer(Dep.Var ~ After.New + Morph.Type + Before + Stress + Sex*Age.Group.Simple + (1 | Speaker), data = td, family = \"binomial\", glmerControl(optCtrl = list(maxfun = 20000), optimizer = \"bobyqa\"))\n\nsummary(td.glmer.sex.age.interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \nDep.Var ~ After.New + Morph.Type + Before + Stress + Sex * Age.Group.Simple +  \n    (1 | Speaker)\n   Data: td\nControl: glmerControl(optCtrl = list(maxfun = 20000), optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n     1117      1188      -545      1089      1175 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.305 -0.492 -0.266  0.492 14.222 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n Speaker (Intercept) 0.695    0.834   \nNumber of obs: 1189, groups:  Speaker, 66\n\nFixed effects:\n                       Estimate Std. Error z value Pr(>|z|)    \n(Intercept)             -0.4344     0.1831   -2.37  0.01767 *  \nAfter.New1               1.7895     0.1554   11.51  < 2e-16 ***\nAfter.New2              -1.0791     0.1371   -7.87  3.6e-15 ***\nMorph.Type1              0.4618     0.1385    3.34  0.00085 ***\nMorph.Type2             -1.7712     0.2055   -8.62  < 2e-16 ***\nBefore1                 -0.3258     0.1695   -1.92  0.05454 .  \nBefore2                  0.6685     0.1870    3.58  0.00035 ***\nBefore3                  0.0159     0.2750    0.06  0.95385    \nBefore4                  0.5365     0.1696    3.16  0.00156 ** \nStress1                 -0.7696     0.1368   -5.63  1.8e-08 ***\nSex1                    -0.2626     0.1359   -1.93  0.05322 .  \nAge.Group.Simple1        0.1281     0.1371    0.93  0.35011    \nSex1:Age.Group.Simple1  -0.1750     0.1363   -1.28  0.19902    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nCorrelation matrix not shown by default, as p = 13 > 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n```\n\n\n:::\n\n```{.r .cell-code}\nAnova(td.glmer.sex.age.interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: Dep.Var\n                      Chisq Df Pr(>Chisq)    \nAfter.New            144.21  2    < 2e-16 ***\nMorph.Type            74.39  2    < 2e-16 ***\nBefore                36.42  4    2.4e-07 ***\nStress                31.67  1    1.8e-08 ***\nSex                    3.71  1      0.054 .  \nAge.Group.Simple       0.71  1      0.399    \nSex:Age.Group.Simple   1.65  1      0.199    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nWhat you can see from the `summary(td.glmer.sex.age.interaction)` and `Anova(td.glmer.sex.age.interaction)` results is that this interaction term is not significant and does not add explanatory value to the analysis.  The negative polarity of the estimate coefficient of the interaction term `Sex1:Age.Group.Simple1` indicates that when the level of `Sex` is `Female` (`1`) and `Age.Group.Simple` is `Old/Middle` (`1`) the overall probability decreases by $-0.1750$. This coefficient represents the extra effect of both predictors working together. The $p$-value of $0.19902$, however, indicates that this change is not statistically different from zero/no effect. In other words, even though we know older men use `Deletion` more frequently, the extra effect of combining age and sex does not emerge as significant when the influence of the linguistic predictors is considered. If you want to home in on the older/middle men in your results, you can reorder the `Sex` predictor. Using the `fct_rev()` function, which reverses the order of factors making the last \"missing\" factor first, is the easiest way to do this. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a regression analysis with a Sex*Age.Group.Simple interaction group in which `Male` equals Sex1\ntd.glmer.sex.age.interaction <- glmer(Dep.Var ~ After.New + Morph.Type + Before + Stress + fct_rev(Sex)*Age.Group.Simple + (1 | Speaker), data = td, family = \"binomial\", glmerControl(optCtrl = list(maxfun = 20000), optimizer = \"bobyqa\"))\n\nsummary(td.glmer.sex.age.interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Dep.Var ~ After.New + Morph.Type + Before + Stress + fct_rev(Sex) *  \n    Age.Group.Simple + (1 | Speaker)\n   Data: td\nControl: glmerControl(optCtrl = list(maxfun = 20000), optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n     1117      1188      -545      1089      1175 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.305 -0.492 -0.266  0.492 14.222 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n Speaker (Intercept) 0.695    0.834   \nNumber of obs: 1189, groups:  Speaker, 66\n\nFixed effects:\n                                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                      -0.4344     0.1831   -2.37  0.01767 *  \nAfter.New1                        1.7895     0.1554   11.51  < 2e-16 ***\nAfter.New2                       -1.0791     0.1371   -7.87  3.6e-15 ***\nMorph.Type1                       0.4618     0.1385    3.34  0.00085 ***\nMorph.Type2                      -1.7712     0.2055   -8.62  < 2e-16 ***\nBefore1                          -0.3258     0.1695   -1.92  0.05454 .  \nBefore2                           0.6685     0.1870    3.58  0.00035 ***\nBefore3                           0.0159     0.2750    0.06  0.95385    \nBefore4                           0.5365     0.1696    3.16  0.00156 ** \nStress1                          -0.7696     0.1368   -5.63  1.8e-08 ***\nfct_rev(Sex)1                     0.2626     0.1359    1.93  0.05323 .  \nAge.Group.Simple1                 0.1281     0.1371    0.93  0.35011    \nfct_rev(Sex)1:Age.Group.Simple1   0.1750     0.1363    1.28  0.19902    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nCorrelation matrix not shown by default, as p = 13 > 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n```\n\n\n:::\n\n```{.r .cell-code}\nAnova(td.glmer.sex.age.interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: Dep.Var\n                               Chisq Df Pr(>Chisq)    \nAfter.New                     144.21  2    < 2e-16 ***\nMorph.Type                     74.39  2    < 2e-16 ***\nBefore                         36.42  4    2.4e-07 ***\nStress                         31.67  1    1.8e-08 ***\nfct_rev(Sex)                    3.71  1      0.054 .  \nAge.Group.Simple                0.71  1      0.399    \nfct_rev(Sex):Age.Group.Simple   1.65  1      0.199    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nYou can see that the coefficient for the interaction term is identical, but with reverse polarity. It indicates that when `Sex` is `Male` and `Age.Group.Simple` is `Old/Middle` the extra effect is $+0.1750$, but again, as you saw above, this difference is not significantly different from zero/no effect $p = 0.19902$.  \n\nAn alternative way to test this interaction is to [create a four-way `Sex:Age.Group.Simple` interaction group](https://lingmethodshub.github.io/content/R/lvc_r/050_lvcr.html) and include it as a fixed effect. By doing this instead of testing for an extra effect caused by the interaction of these two variables, you are instead testing the difference in likelihood from the overall likelihood for each combination of age and sex, and determining whether this is significantly different from zero. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a four-way interaction group\ntd <-td %>% unite(\"Sex.Age.Group.Simple\", c(Sex, Age.Group.Simple), sep= \":\", remove = FALSE)\nlevels(as.factor(td$Sex.Age.Group.Simple))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"F:Old/Middle\" \"F:Young\"      \"M:Old/Middle\" \"M:Young\"     \n```\n\n\n:::\n:::\n\nThe levels of the four-way interaction group are `F:Old/Middle`, `F:Young`, `M:Old/Middle`, and `M:Young`. If you recreate your `glmer()` analysis, you should find that the third level, `M:Old/Middle`, to have a positive coefficient (`Deletion` more likely than the mean), and the others to have a negative coefficient (`Deletion` less likely than the mean). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a regression analysis with the Age.Simple:Sex interaction group \ntd.glmer.sex.age.interaction <- glmer(Dep.Var ~ After.New + Morph.Type + Before + Stress + Sex.Age.Group.Simple + (1 | Speaker), data = td, family = \"binomial\", glmerControl(optCtrl = list(maxfun = 20000), optimizer = \"bobyqa\"))\n\nsummary(td.glmer.sex.age.interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \nDep.Var ~ After.New + Morph.Type + Before + Stress + Sex.Age.Group.Simple +  \n    (1 | Speaker)\n   Data: td\nControl: glmerControl(optCtrl = list(maxfun = 20000), optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n     1117      1188      -545      1089      1175 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.305 -0.492 -0.266  0.492 14.222 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n Speaker (Intercept) 0.695    0.834   \nNumber of obs: 1189, groups:  Speaker, 66\n\nFixed effects:\n                      Estimate Std. Error z value Pr(>|z|)    \n(Intercept)            -0.4344     0.1831   -2.37  0.01767 *  \nAfter.New1              1.7895     0.1554   11.51  < 2e-16 ***\nAfter.New2             -1.0791     0.1371   -7.87  3.6e-15 ***\nMorph.Type1             0.4618     0.1385    3.34  0.00085 ***\nMorph.Type2            -1.7712     0.2055   -8.62  < 2e-16 ***\nBefore1                -0.3258     0.1695   -1.92  0.05454 .  \nBefore2                 0.6685     0.1870    3.58  0.00035 ***\nBefore3                 0.0159     0.2750    0.06  0.95386    \nBefore4                 0.5365     0.1696    3.16  0.00156 ** \nStress1                -0.7696     0.1368   -5.63  1.8e-08 ***\nSex.Age.Group.Simple1  -0.3095     0.2161   -1.43  0.15206    \nSex.Age.Group.Simple2  -0.2158     0.2441   -0.88  0.37668    \nSex.Age.Group.Simple3   0.5658     0.2557    2.21  0.02692 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nCorrelation matrix not shown by default, as p = 13 > 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n```\n\n\n:::\n\n```{.r .cell-code}\nAnova(td.glmer.sex.age.interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: Dep.Var\n                      Chisq Df Pr(>Chisq)    \nAfter.New            144.21  2    < 2e-16 ***\nMorph.Type            74.39  2    < 2e-16 ***\nBefore                36.42  4    2.4e-07 ***\nStress                31.67  1    1.8e-08 ***\nSex.Age.Group.Simple   5.62  3       0.13    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nBy using this four-way interaction group you can see that the `M:Older/Middle` (`Sex.Age.Group.Simple3`) coefficient is negative, and it is significantly different from zero/no effect. To find the coefficient for the missing fourth value, re-create the analysis using `fct_rev()`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a regression analysis with the reversed Age.Simple:Sex interaction group \ntd.glmer.sex.age.interaction <- glmer(Dep.Var ~ After.New + Morph.Type + Before + Stress + fct_rev(Sex.Age.Group.Simple) + (1 | Speaker), data = td, family = \"binomial\", glmerControl(optCtrl = list(maxfun = 20000), optimizer = \"bobyqa\"))\n\nsummary(td.glmer.sex.age.interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \nDep.Var ~ After.New + Morph.Type + Before + Stress + fct_rev(Sex.Age.Group.Simple) +  \n    (1 | Speaker)\n   Data: td\nControl: glmerControl(optCtrl = list(maxfun = 20000), optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n     1117      1188      -545      1089      1175 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.305 -0.492 -0.266  0.492 14.222 \n\nRandom effects:\n Groups  Name        Variance Std.Dev.\n Speaker (Intercept) 0.695    0.834   \nNumber of obs: 1189, groups:  Speaker, 66\n\nFixed effects:\n                               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                     -0.4344     0.1831   -2.37  0.01767 *  \nAfter.New1                       1.7895     0.1554   11.51  < 2e-16 ***\nAfter.New2                      -1.0791     0.1371   -7.87  3.6e-15 ***\nMorph.Type1                      0.4618     0.1385    3.34  0.00085 ***\nMorph.Type2                     -1.7712     0.2055   -8.62  < 2e-16 ***\nBefore1                         -0.3258     0.1695   -1.92  0.05454 .  \nBefore2                          0.6685     0.1870    3.58  0.00035 ***\nBefore3                          0.0159     0.2750    0.06  0.95386    \nBefore4                          0.5365     0.1696    3.16  0.00156 ** \nStress1                         -0.7696     0.1368   -5.63  1.8e-08 ***\nfct_rev(Sex.Age.Group.Simple)1  -0.0405     0.2273   -0.18  0.85861    \nfct_rev(Sex.Age.Group.Simple)2   0.5658     0.2557    2.21  0.02692 *  \nfct_rev(Sex.Age.Group.Simple)3  -0.2158     0.2441   -0.88  0.37667    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nCorrelation matrix not shown by default, as p = 13 > 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n```\n\n\n:::\n\n```{.r .cell-code}\nAnova(td.glmer.sex.age.interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: Dep.Var\n                               Chisq Df Pr(>Chisq)    \nAfter.New                     144.21  2    < 2e-16 ***\nMorph.Type                     74.39  2    < 2e-16 ***\nBefore                         36.42  4    2.4e-07 ***\nStress                         31.67  1    1.8e-08 ***\nfct_rev(Sex.Age.Group.Simple)   5.62  3       0.13    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nAs with the other levels, `Men:Young` (`fct_rev(Sex.Age.Group.Simple)1`) is not significant. What you can conclude is that all women and young men are not significantly different from the overall probability (and by extension each other), but old/middle men are significantly different from the overall probability. Creating this four-way interaction group and including it as a fixed effect reveals this pattern in a way that including the interaction term `Sex:Age.Simple` does not. That being said, the results of the `Anova()` indicate that this predictor still does not add explanatory value to the analysis.  \n\n\n\n### Collinearity \n\nWhen fixed effects predictors are not independent we say they are (multi-) [collinear](https://www.britannica.com/topic/collinearity-statistics). Collinearity and interactions are similar, but separate, phenomena. \n\nCollinearity is the phenomenon whereby two or more predictor variables are highly correlated, such that the value/level of one can be predicted from the value/level of the other with a non-trivial degree of accuracy. Including both in an anlysis 1) violates the assumptions of the model; and 2) can actually result in diminished liklihood estimates for both predictors, masking real effects. As discussed [before](https://lingmethodshub.github.io/content/R/lvc_r/080_lvcr.html), in this Cape Breton data, `Education`, `Job`, and `Age.Group` are all collinear to various degrees. For example, if `Job` is `Student`, then the level of  `Education` can be predicted (it will also be `Student`), and vice versa. For both, `Age.Group` can be predicted too (`Young`). These types of correlations are easy to see for social categories, but somewhat more difficult to tease out for linguistic categories. The first step is always a thorough [cross tabulation](https://lingmethodshub.github.io/content/R/lvc_r/060_lvcr.html) of your independent variables. For a quick visual of cross tabulations you can employ a [mosaic plot](https://cran.r-project.org/web/packages/ggmosaic/vignettes/ggmosaic.html). Here is the code for creating a quick mosaic plot using `ggplot2`, which you likely already have installed if you've followed along with previous chapters. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install ggmosaic package\ninstall.packages(\"ggmosaic\")\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggmosaic)\nlibrary(ggplot2)\n# Create a quick mosaic plot of Phoneme and Before\nggplot(td)+\n  geom_mosaic(aes(product(Dep.Var, Before, Phoneme), fill = Dep.Var))+\n  theme_mosaic()\n```\n\n::: {.cell-output-display}\n![](114_lvcr_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-pos='H' width=75%}\n:::\n:::\n\n\nIn a mosaic plot the size of the box for each combination of variables corresponds to the relative number of tokens of that combination. The first major observation from the mosaic plot is that there are no preceding /s/ tokens where the underlying phoneme is /d/, and there are remarkably few tokens in which /d/ is preceded by a non-nasal stop.[^3] This indicates that you shouldn't include both of these predictors in your model, or complexify the model by creating a interaction group of `Phoneme` and `Before`. This is because you can predict some of the values of `Phoneme` and `Before`. For example, if a token is underlyingly world-final /d/, the preceding segment will not be /s/. Likewise, if the preceding segment is /s/, the underlying phoneme must be /t/.\n\n[^3]: I checked the data and there are only three such tokens: one token of *bugged* and two of *hugged*. \n\nLets look at another mosaic plot. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a quick mosaic plot of Phoneme and Before\nggplot(td)+\n  geom_mosaic(aes(product(Dep.Var, Before, After.New), fill = Dep.Var))+\n  theme_mosaic()\n```\n\n::: {.cell-output-display}\n![](114_lvcr_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H' width=75%}\n:::\n:::\n\n\nThis plot is a little bit hard to read. To make the *x*-axis labels a little easier to read, you can use the binary version of `Dep.Var`. You'll remember that `1` is `Deletion` and `0` is `Realized`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a quick mosaic plot of Phoneme and Before\nggplot(td)+\n  geom_mosaic(aes(product(Dep.Var.Binary, Before, After.New), fill = Dep.Var.Binary))+\n  theme_mosaic()\n```\n\n::: {.cell-output-display}\n![](114_lvcr_files/figure-pdf/unnamed-chunk-11-1.pdf){fig-pos='H' width=75%}\n:::\n:::\n\n\nThis mosaic plot show that there are very few tokens with a preceding `Stop` and a following `Pause`. Though no apparent collinearity is present, this mosaic plot does reveal some interesting potential interactions. The effect of preceding /s/, liquids, and nasals appears to be specific to pre-consonental contexts, and perhaps also pre-vowel contexts for nasals. This suggests further exploration of the data is warranted --- perhaps by creating separate `glmer()` models for each following context, by complexifying your full model by creating a `Before` and `After.New` interaction group, or by simplifying your full model by [collapsing these two categories](https://lingmethodshub.github.io/content/R/lvc_r/040_lvcr.html) into simpler grouped categories, e.g., `Pre-Pause`, `Liquid-Consonant`, `Liquid-Vowel`, `Nasal-Consonant`, `Nasal-Vowel`, `S-Consonant`, `S-Vowel`, `Other-Consonant`, `Other-Vowel`, or similar.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a new model using insights from the mosaic plots\ntd<-td %>%\n    mutate(Before.After = factor(paste(td$Before, td$After.New, sep =\".\")))%>%\n    mutate(Before.After = recode_factor(Before.After, \n                                 \"Liquid.Pause\" = \"Pause\",\n                                 \"Nasal.Pause\" = \"Pause\",\n                                 \"Other Fricative.Pause\" = \"Pause\", \n                                 \"S.Pause\" = \"Pause\", \n                                 \"Stop.Pause\" = \"Pause\", \n                                 \"Other Fricative.Consonant\" = \"Other.Consonant\",\n                                 \"Stop.Consonant\" = \"Other.Consonant\", \n                                 \"Other Fricative.Vowel\"= \"Other.Vowel\", \n                                 \"Stop.Vowel\" = \"Other.Vowel\"))\n\n\ntd.glmer.parsimonious.new <- glmer(Dep.Var ~ Before.After + Morph.Type + Stress + (1 | Speaker), data = td, family = \"binomial\", glmerControl(optCtrl = list(maxfun = 20000), optimizer = \"bobyqa\"))\n\n# Compare fit of new parsimonious model with old parsimonious model\nanova(td.glmer.parsimonious, td.glmer.parsimonious.new)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: td\nModels:\ntd.glmer.parsimonious: Dep.Var ~ After.New + Morph.Type + Before + Stress + Phoneme + (1 | Speaker)\ntd.glmer.parsimonious.new: Dep.Var ~ Before.After + Morph.Type + Stress + (1 | Speaker)\n                          npar  AIC  BIC logLik -2*log(L) Chisq Df Pr(>Chisq)\ntd.glmer.parsimonious       12 1114 1175   -545      1090                    \ntd.glmer.parsimonious.new   13 1087 1153   -531      1061  28.6  1    8.7e-08\n                             \ntd.glmer.parsimonious        \ntd.glmer.parsimonious.new ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nThe `anova()` function shows that `td.glmer.parsimonious.new` is a better fit model. In other words, it does a better job of predicting the variation in the data. \n\n\n\nIt is important, however,  to point out that collinearity does not reduce the predictive power or reliability of the `glmer()` model as a whole --- it only affects calculations regarding individual predictors. That is, a `glmer()` model with correlated predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others. In other words, collinearity prevents you from discovering the three lines of evidence. \n\n\nSo how can you test whether your predictors are collinear? There are two measures beyond just looking at the correlation matrix (which can point to either collinearity or interaction).\n\n\nThe first method to test collinearity is to find the **Condition Number** ($\\kappa$ a.k.a. kappa). The function to calculate this comes from a [package](https://rdrr.io/github/jasongraf1/JGmermod/) created by linguist Jason Grafmiller, which adapts the `collin.fnc()` function from Baayen's [`languageR`](https://cran.r-project.org/web/packages/languageR/index.html) package to work with `lme4` mixed models. To install this package you need to first install the `devtools()` package, and then you can download it. We will return to the original `td.glmer.parsimonious` to do this test. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install JGermod\ninstall.packages(\"devtools\")\ndevtools::install_github(\"jasongraf1/JGmermod\")\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(JGmermod)\n# Calculate Condition Number\ncollin.fnc.mer(td.glmer.parsimonious)$cnumber\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.2\n```\n\n\n:::\n:::\n\n\nThe Condition Number here is less than $6$ indicating no collinearity @Baayen2008 [p. 182]. According to Baayen [citing @Belsley1980], when the condition number is between $0$ and $6$, there is no collinearity to speak of. Medium collinearity is indicated by condition numbers around $15$, and condition numbers of $30$ or more indicate potentially harmful collinearity. \n\nThe second measure of collinearity is determining the **Variable Inflation Factor** (VIF), which estimates how much the variance of a regression coefficient is inflated due to (multi)collinearity. The function `check_collinearity()` from the `performance` package is used to calculate the VIF. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"performance\")\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(performance)\ncheck_collinearity(td.glmer.parsimonious)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Check for Multicollinearity\n\nLow Correlation\n\n       Term  VIF   VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n  After.New 2.68 [2.45, 2.94]     1.64      0.37     [0.34, 0.41]\n Morph.Type 2.06 [1.90, 2.25]     1.44      0.49     [0.44, 0.53]\n     Before 4.93 [4.46, 5.46]     2.22      0.20     [0.18, 0.22]\n     Stress 1.68 [1.56, 1.83]     1.30      0.59     [0.55, 0.64]\n    Phoneme 1.87 [1.73, 2.04]     1.37      0.53     [0.49, 0.58]\n```\n\n\n:::\n:::\n\nAccording to the [`performance` package documentation](https://rdrr.io/cran/performance/man/check_collinearity.html), a `VIF` less than $5$ indicates a low correlation of that predictor with other predictors. A value between $5$ and $10$ indicates a moderate correlation, while VIF values larger than $10$ are a sign for high, not tolerable correlation of model predictors [@James2013]. The `Increased SE` column in the output indicates how much larger the standard error is due to the association with other predictors conditional on the remaining variables in the model. \n\nBased on the Condition Number ($\\kappa<6$) and the VIF ($<5$) you can report that any (multi-) collinearity in your model is within acceptably low limits. You can add this to your manuscript table, as in Table 1 (based on `td.glmer.parsimonious`), though you should always contextualize what these measures indicate (i.e., low collinearity) in the text too.\n\n::: {.content-visible when-format=\"html\"}\n\n![Mixed-effects logistic regression testing the fixed effect of Following Context,  Morpheme Type, Preceding Context, Stress and Phoneme and a random intercept of Speaker on the deletion of word-final (t, d) in Cape Breton English](images/lme4table2.png){#fig-lme4table2 width=\"80%\"}\n\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n```{=latex}\n  \\begin{table}[h]\n\\noindent\n\\begin{center}\n\\begin{threeparttable}\n\\caption{Mixed-effects logistic regression testing the fixed effect of \\textsc{Following Context},  \\textsc{Morpheme Type}, \\textsc{Preceding Context}, \\textsc{Stress} and \\textsc{Phoneme} and a random intercept of \\emph{Speaker} on the deletion of word-final \\textipa{/t, d/} in Cape Breton English}\n\\label{tab:treatmentglmer}\n\\begin{tabular}{lrrrcrc}\n\\toprule\n\\multicolumn{5}{l}{AIC = 1114, Marginal $R^2$ = .40, Conditional $R^2$ = .52}&\\multicolumn{2}{c}{Observations}\\\\\n\\cmidrule(lr){6-7} \nFixed Effects: & \\multicolumn{1}{c}{Estimate} & \\multicolumn{1}{c}{Std. Error}&\\multicolumn{1}{c}{\\textit{z}-value}&\\multicolumn{1}{c}{\\textit{p}-value} &\\multicolumn{1}{c}{\\textit{n}}&\\multicolumn{1}{c}{\\% Deletion} \\\\\n\\midrule\n\\textsc{Intercept} (Grand Mean) & -0.277 & 0.207 & -1.34 &&1,189 &32\\\\\n\\textsc{Following Context} &&&&&\\\\\n\\quad\\textit{Consonant} & 1.840&0.157&11.71&$\\ast$$\\ast$$\\ast$ & 372 & 54\\\\\n\\quad\\textit{Vowel} & -0.665&0.161&-4.13&$\\ast$$\\ast$$\\ast$ & 259 & 28\\\\\n\\quad\\textit{Pause} & -1.175&0.144&-8.14&$\\ast$$\\ast$$\\ast$ & 558 & 20\\\\\n\\textsc{Morpheme Type} &&&&&\\\\\n\\quad\\textit{Semi-Weak Simple Past}&1.466&0.207&7.10&$\\ast$$\\ast$$\\ast$&116&63\\\\\n\\quad\\textit{Mono-morpheme} & 0.426&0.140&3.05&$\\ast$$\\ast$$\\ast$ & 762 & 37\\\\\n\\quad\\textit{Weak Simple Past} & -1.892&0.213&-8.87&$\\ast$$\\ast$$\\ast$ & 311 & 10\\\\\n\\textsc{Stress} &&&&&\\\\\n\\quad\\textit{Unstressed}&0.799&0.137&5.81  &$\\ast$$\\ast$$\\ast$ & 142 & 47\\\\\n\\quad\\textit{Stressed} & -1.598&0.275&-5.81&$\\ast$$\\ast$$\\ast$ & 1,047 & 31\\\\\n\\textsc{Preceding Context} &&&&&\\\\\n\\quad\\textit{\\textipa{/s/}}&0.731&0.190&3.85&$\\ast$$\\ast$$\\ast$ &332 & 53\\\\\n\\quad\\textit{Nasal} & 0.526&0.193&2.72&$\\ast$$\\ast$ & 209 & 39\\\\\n\\quad\\textit{Other Fricative} & 0.117&0.278&0.42&& 130 & 15\\\\\n\\quad\\textit{Liquid} & -0.575  &0.202&-2.84&$\\ast$$\\ast$ & 269 & 42\\\\\n\\quad\\textit{Stop} & -0.799&0.189&-4.22&$\\ast$$\\ast$$\\ast$ & 249 & 27\\\\\n\\textsc{Phoneme} &&&&&\\\\\n\\quad\\textit{\\textipa{/d/}}&0.287&0.128&2.25&$\\ast$&878 &34\\\\\n\\quad\\textit{\\textipa{/t/}} &  -0.287&0.128&-2.25&$\\ast$ & 311 & 29\\\\\n\\midrule\n\\multicolumn{5}{l}{Random Effects:} & \\textit{sd} & \\textit{n}\\\\\n\\midrule\n\\textsc{Speaker} &&&&& 0.892&  66\\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tablenotes}\n\\item \\hfill$\\ast\\ast\\ast$~$p<0.001$,  $\\ast\\ast$~$p<0.01$, $\\ast$~$p<0.05$\\\\[-10pt]\n\\item  Sum contrast coding. Estimate coefficients reported in log-odds. \n\\item Model significantly better than null model (AIC = 1,456, $\\chi^2$ = 362, df = 10, $\\ast\\ast\\ast$)\n\\item Correlation of Fixed Effects $\\le|0.54|$, $\\kappa = 5.2$, Variable Inflation Factor $\\le4.93$ \n\\end{tablenotes}\n\\end{threeparttable}\n\\end{center}\n\\end{table} \n```\n\n:::\n\n\n\nKeep in mind that if there are interaction terms (e.g., `Sex*Age.Group`) in your model high VIF values are expected. This is because you are explicitly expecting and testing a correlation between two predictors. The (multi-) collinearity among two components of the interaction term is also called \"inessential ill-conditioning\", which leads to inflated VIF values.\n\nAlso keep in mind that (multi-) collinearity might arise when a third, unobserved variable has a causal effect on two or more predictors' effect on the dependant variable. For example, correlated Education and Job Type effects may be caused by an underlying age effect, if older speakers are generally less educated and blue-collar workers and young speakers are generally more educated and white collar workers.  In such cases, the actual relationship that matters is the association between the unobserved variable and the dependant variable. If confronted with a case like this, you should revisit what independent predictors are included in the model. Non-inferential tools that can include (multi-) collinear descriptors (like [Conditional Inference Trees](https://lingmethodshub.github.io/content/R/lvc_r/080_lvcr.html) or [Random Forests](https://lingmethodshub.github.io/content/R/lvc_r/090_lvcr.html)) may help you.    \n\n### References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "114_lvcr_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}