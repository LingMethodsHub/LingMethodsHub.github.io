{
  "hash": "bb55d6c152848a5a1c4cb95e0c72c768",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"How to normalize your vowels using the tidyverse\"\nformat: html\neditor: visual\nknitr: \n  opts_chunk: \n    message: false\ndescription: \"A tutorial on how to normalize vowel formant measurments in R using the tidyverse\"\nbibliography: references.bib\ncategories:\n  - \"vowels\"\n  - \"normalization\"\n  - \"R\"\n---\n\n\n::: {.cell}\n\n:::\n\n\nThis is a tutorial about how to \"normalize\" vowel formant data using data tools from the [tidyverse](https://www.tidyverse.org/). A *lot* has been written about vowel normalization (why we do it, how it should work, what methods are best) that I can't really cover here, although @Adank2004 is often taken as the canonical citation, and I'll be taking into account the recent \"order of operations\" from @stanley2022.\n\nThere's also a `vowels` R package [@vowels] that basically lets you run [the NORM suite locally](http://lingtools.uoregon.edu/norm/norm1_methods.php).\n\nThere are still many occasions when you might want or need to normalize your vowel data yourself, though, and learning how to do it is actually a great introduction to a number of tidyverse \"verbs\", especially:\n\n-   [`dplyr::group_by()`](https://dplyr.tidyverse.org/reference/group_by.html)\n\n-   [`dplyr::summarise()`](https://dplyr.tidyverse.org/reference/summarise.html)\n\n-   [`dplyr::mutate()`](https://dplyr.tidyverse.org/reference/mutate.html)\n\n-   [`tidyr::pivot_longer()`](https://tidyr.tidyverse.org/reference/pivot_longer.html)\n\n-   [`tidyr::pivot_wider()`](https://tidyr.tidyverse.org/reference/pivot_wider.html)\n\nSince I'm focusing on how data structures relate to vowel normalization, I'll cover 3 specific normalization procedures:\n\n### Lobanov (a.k.a. z-scoring)\n\nIn terms of tidy data procedures, this is the simplest. All it requires is a `group_by()` and a `mutate()`\n\n### Nearey 2\n\nThis procedure is a little more complicated, involving \"pivoting\" our data from wide to long, then long to wide again, using `pivot_longer()` and `pivot_wider()`.\n\n### Watt & Fabricius\n\nThis method requires estimating by-speaker scaling factors (using `group_by()` and `summarise()`) then merging them back onto the original data (with `left_join()`).\n\n## Setup\n\nTo begin with, I'm going to import a few packages:\n\n-   `tidyverse`: This is a \"metapackage\" that imports many different packages that contain functions we'll need, including `ggplot2`\n\n-   `ggforce`: This is a package that extends some of `ggplot2`'s functionality\n\n-   `khroma`: This is another packages that has many different color palates for `ggplot2`.\n\n-   `joeyr`: This is a package with functions written by Joey Stanley for vowel analysis.\n\nAny time I use a function that's not loaded by the tidyverse, I'll indicate it with the `package::function()` convention.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.5.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tibble' was built under R version 4.5.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tidyr' was built under R version 4.5.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'purrr' was built under R version 4.5.2\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggforce)\nlibrary(khroma)\n# remotes::install_github(\"JoeyStanley/joeyr\")\nlibrary(joeyr)\n\n# set the ggplot2 theme\ntheme_set(theme_minimal())\n```\n:::\n\n\nWe also need to load in some data. Here are two tab-delimited files of [Buckeye Corpus](https://buckeyecorpus.osu.edu/) speakers whose interviews were run through [FAVE](https://github.com/JoFrhwld/FAVE).\n\n``` r\ns01_url <- \"{{< meta website.site-url >}}/content/R/tidy-norm/data/s01.txt\"\ns03_url <- \"{{< meta website.site-url >}}/content/R/tidy-norm/data/s03.txt\"\n```\n\n\n::: {.cell}\n\n:::\n\n\nThis code should load these files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvowels_orig <- map_dfr(c(s01_url, s03_url), \n                       ~read_tsv(.x, col_types=cols(sex = 'c')))\n```\n:::\n\n\n<details>\n\n<summary>What that `map_dfr` thing did:</summary>\n\nThe `map_dfr` function iterated over each url, and applied the function `read_tsv()` to them. I had to tell `read_tsv()` that the column `sex` should be treated as a character data type, since the value `f` gets reinterpreted as `FALSE` otherwise. After reading in each tab-delimited file, `map_dfr()` then combines the results together row-wise, to produce one large data frame.\n\n</details>\n\nThe variable `vowels_orig` is now one large data frame with both speakers' FAVE output in it. Here's three randomly sampled rows from each speakers' data.\n\n\n::: {#tbl-vowel-orig .cell .tbl-cap-location-bottom tbl-cap='Three randomly sampled rows from each speaker\\'s data'}\n\n```{.r .cell-code}\nset.seed(50)\nvowels_orig |>\n  group_by(name) |>\n  slice_sample(n = 3) |>\n  rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"age\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"city\"],\"name\":[2],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"ethnicity\"],\"name\":[3],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"first_name\"],\"name\":[4],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"last_name\"],\"name\":[5],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"location\"],\"name\":[6],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"name\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"sex\"],\"name\":[8],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"state\"],\"name\":[9],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"tiernum\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"year\"],\"name\":[11],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"years_of_schooling\"],\"name\":[12],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"vowel\"],\"name\":[13],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"stress\"],\"name\":[14],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"pre_word\"],\"name\":[15],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[16],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"fol_word\"],\"name\":[17],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"F1\"],\"name\":[18],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F2\"],\"name\":[19],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F3\"],\"name\":[20],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"B1\"],\"name\":[21],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"B2\"],\"name\":[22],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"B3\"],\"name\":[23],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"t\"],\"name\":[24],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"beg\"],\"name\":[25],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"end\"],\"name\":[26],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"dur\"],\"name\":[27],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"plt_vclass\"],\"name\":[28],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ipa_vclass\"],\"name\":[29],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"plt_manner\"],\"name\":[30],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"plt_place\"],\"name\":[31],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"plt_voice\"],\"name\":[32],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"plt_preseg\"],\"name\":[33],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"plt_folseq\"],\"name\":[34],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"style\"],\"name\":[35],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"glide\"],\"name\":[36],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"pre_seg\"],\"name\":[37],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"fol_seg\"],\"name\":[38],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"context\"],\"name\":[39],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"vowel_index\"],\"name\":[40],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"pre_word_trans\"],\"name\":[41],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word_trans\"],\"name\":[42],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"fol_word_trans\"],\"name\":[43],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"F1@20%\"],\"name\":[44],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F2@20%\"],\"name\":[45],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F1@35%\"],\"name\":[46],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F2@35%\"],\"name\":[47],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F1@50%\"],\"name\":[48],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F2@50%\"],\"name\":[49],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F1@65%\"],\"name\":[50],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F2@65%\"],\"name\":[51],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F1@80%\"],\"name\":[52],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F2@80%\"],\"name\":[53],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"nFormants\"],\"name\":[54],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"y\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"s01\",\"8\":\"f\",\"9\":\"NA\",\"10\":\"0\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"IY\",\"14\":\"0\",\"15\":\"THESE\",\"16\":\"REALLY\",\"17\":\"ARE\",\"18\":\"555.3\",\"19\":\"2140.4\",\"20\":\"3260.0\",\"21\":\"237.0\",\"22\":\"149.0\",\"23\":\"310.5\",\"24\":\"2677.844\",\"25\":\"2677.811\",\"26\":\"2677.911\",\"27\":\"0.10\",\"28\":\"iyF\",\"29\":\"iF\",\"30\":\"NA\",\"31\":\"NA\",\"32\":\"NA\",\"33\":\"liquid\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"L\",\"38\":\"AA1\",\"39\":\"final\",\"40\":\"4\",\"41\":\"DH IY1 Z\",\"42\":\"R IH1 L IY0\",\"43\":\"AA1 R\",\"44\":\"605.8\",\"45\":\"1935.0\",\"46\":\"568.2\",\"47\":\"2100.6\",\"48\":\"528.1\",\"49\":\"2216.9\",\"50\":\"591.2\",\"51\":\"2161.8\",\"52\":\"645.2\",\"53\":\"2073.1\",\"54\":\"5\"},{\"1\":\"y\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"s01\",\"8\":\"f\",\"9\":\"NA\",\"10\":\"0\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"AH\",\"14\":\"0\",\"15\":\"WONDERFUL\",\"16\":\"REPUTATION\",\"17\":\"AN\",\"18\":\"602.3\",\"19\":\"2017.2\",\"20\":\"3149.3\",\"21\":\"145.5\",\"22\":\"308.1\",\"23\":\"126.9\",\"24\":\"735.287\",\"25\":\"735.264\",\"26\":\"735.334\",\"27\":\"0.07\",\"28\":\"@\",\"29\":\"ə\",\"30\":\"nasal\",\"31\":\"apical\",\"32\":\"voiced\",\"33\":\"palatal\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"SH\",\"38\":\"N\",\"39\":\"internal\",\"40\":\"9\",\"41\":\"W AH1 N D ER0 F AH0 L\",\"42\":\"R EH2 P Y AH0 T EY1 SH AH0 N\",\"43\":\"AH0 N\",\"44\":\"962.4\",\"45\":\"2333.7\",\"46\":\"661.2\",\"47\":\"2092.4\",\"48\":\"585.0\",\"49\":\"1872.9\",\"50\":\"573.0\",\"51\":\"1934.5\",\"52\":\"563.2\",\"53\":\"1923.1\",\"54\":\"5\"},{\"1\":\"y\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"s01\",\"8\":\"f\",\"9\":\"NA\",\"10\":\"0\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"EH\",\"14\":\"1\",\"15\":\"QUITE\",\"16\":\"WELL\",\"17\":\"NOW\",\"18\":\"709.6\",\"19\":\"1388.0\",\"20\":\"2788.7\",\"21\":\"171.5\",\"22\":\"194.6\",\"23\":\"183.9\",\"24\":\"584.777\",\"25\":\"584.757\",\"26\":\"584.817\",\"27\":\"0.06\",\"28\":\"e\",\"29\":\"ɛ\",\"30\":\"lateral\",\"31\":\"apical\",\"32\":\"voiced\",\"33\":\"w/y\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"W\",\"38\":\"L\",\"39\":\"internal\",\"40\":\"2\",\"41\":\"K W AY1 T\",\"42\":\"W EH1 L\",\"43\":\"N AW1\",\"44\":\"635.1\",\"45\":\"1314.8\",\"46\":\"635.1\",\"47\":\"1314.8\",\"48\":\"777.8\",\"49\":\"1445.7\",\"50\":\"803.7\",\"51\":\"1455.2\",\"52\":\"804.4\",\"53\":\"1451.3\",\"54\":\"5\"},{\"1\":\"o\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"s03\",\"8\":\"m\",\"9\":\"NA\",\"10\":\"0\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"ER\",\"14\":\"1\",\"15\":\"TODAY\",\"16\":\"VERSUS\",\"17\":\"WHAT\",\"18\":\"409.7\",\"19\":\"1251.9\",\"20\":\"3475.6\",\"21\":\"106.6\",\"22\":\"119.1\",\"23\":\"326.0\",\"24\":\"1993.785\",\"25\":\"1993.742\",\"26\":\"1993.872\",\"27\":\"0.13\",\"28\":\"*hr\",\"29\":\"ə˞\",\"30\":\"fricative\",\"31\":\"apical\",\"32\":\"voiceless\",\"33\":\"oral_labial\",\"34\":\"one_fol_syll\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"V\",\"38\":\"S\",\"39\":\"internal\",\"40\":\"2\",\"41\":\"T AH0 D EY1\",\"42\":\"V ER1 S AH0 S\",\"43\":\"W AH1 T\",\"44\":\"607.3\",\"45\":\"2460.1\",\"46\":\"411.1\",\"47\":\"1261.4\",\"48\":\"408.2\",\"49\":\"1225.1\",\"50\":\"351.9\",\"51\":\"1299.7\",\"52\":\"1327.9\",\"53\":\"3962.3\",\"54\":\"3\"},{\"1\":\"o\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"s03\",\"8\":\"m\",\"9\":\"NA\",\"10\":\"0\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"AA\",\"14\":\"2\",\"15\":\"WELL\",\"16\":\"GRANDPA\",\"17\":\"HOW\",\"18\":\"536.1\",\"19\":\"857.6\",\"20\":\"2272.3\",\"21\":\"134.7\",\"22\":\"111.2\",\"23\":\"335.8\",\"24\":\"3291.208\",\"25\":\"3291.151\",\"26\":\"3291.321\",\"27\":\"0.17\",\"28\":\"o\",\"29\":\"ɑ\",\"30\":\"NA\",\"31\":\"NA\",\"32\":\"NA\",\"33\":\"oral_labial\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"P\",\"38\":\"HH\",\"39\":\"final\",\"40\":\"6\",\"41\":\"W EH1 L\",\"42\":\"G R AE1 M P AA2\",\"43\":\"HH AW1\",\"44\":\"444.9\",\"45\":\"893.9\",\"46\":\"533.6\",\"47\":\"861.6\",\"48\":\"554.7\",\"49\":\"829.6\",\"50\":\"508.0\",\"51\":\"853.2\",\"52\":\"620.7\",\"53\":\"1022.6\",\"54\":\"6\"},{\"1\":\"o\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"s03\",\"8\":\"m\",\"9\":\"NA\",\"10\":\"0\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"IH\",\"14\":\"0\",\"15\":\"OF\",\"16\":\"THIS\",\"17\":\"MASS\",\"18\":\"429.4\",\"19\":\"1460.3\",\"20\":\"2851.0\",\"21\":\"61.9\",\"22\":\"244.1\",\"23\":\"635.2\",\"24\":\"936.169\",\"25\":\"936.136\",\"26\":\"936.236\",\"27\":\"0.10\",\"28\":\"i\",\"29\":\"ɪ\",\"30\":\"fricative\",\"31\":\"apical\",\"32\":\"voiceless\",\"33\":\"oral_apical\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"DH\",\"38\":\"S\",\"39\":\"internal\",\"40\":\"2\",\"41\":\"AH0 V\",\"42\":\"DH IH0 S\",\"43\":\"M AE1 S\",\"44\":\"420.9\",\"45\":\"1372.7\",\"46\":\"428.0\",\"47\":\"1441.2\",\"48\":\"427.1\",\"49\":\"1487.8\",\"50\":\"415.2\",\"51\":\"1517.5\",\"52\":\"385.8\",\"53\":\"1482.9\",\"54\":\"5\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nFAVE outputs a lot of useful information, but for this tutorial, I want to narrow down our focus to just a few columns\n\n-   `name`: We'll use this as a unique ID for each speaker\n\n-   `word`: It's just good to keep this info around\n\n-   `ipa_vclass`: A column indicating each token's vowel class in an IPA-like format\n\n-   `F1`, `F2`: What we're all here for. The first and second formants.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvowels_orig |>\n  select(name, word, ipa_vclass, F1, F2) -> vowels_focus\n```\n:::\n\n\n\n::: {#tbl-vowel-focus .cell tbl-cap='The first few rows of the data we\\'re going to work with'}\n\n```{.r .cell-code}\nvowels_focus |>\n  head() |>\n  rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ipa_vclass\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"F1\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F2\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"s01\",\"2\":\"OKAY\",\"3\":\"ejF\",\"4\":\"763.5\",\"5\":\"2088.1\"},{\"1\":\"s01\",\"2\":\"UM\",\"3\":\"ʌ\",\"4\":\"699.5\",\"5\":\"1881.3\"},{\"1\":\"s01\",\"2\":\"I'M\",\"3\":\"aj\",\"4\":\"888.8\",\"5\":\"1934.1\"},{\"1\":\"s01\",\"2\":\"LIVED\",\"3\":\"ɪ\",\"4\":\"555.5\",\"5\":\"1530.5\"},{\"1\":\"s01\",\"2\":\"IN\",\"3\":\"ɪ\",\"4\":\"612.2\",\"5\":\"2323.4\"},{\"1\":\"s01\",\"2\":\"COLUMBUS\",\"3\":\"ə\",\"4\":\"612.4\",\"5\":\"1903.7\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## Unnormalized\n\nFirst, let's see how things look when we get our vowel means and plot them unnormalized. I won't go into detail about how the plotting code works (see the LingMethodsHub tutorial on [ggplot2 vowel plots](https://lingmethodshub.github.io/content/R/vowel-plots-tutorial/)).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plot code\"}\nvowels_focus |>\n  group_by(name, ipa_vclass) |>\n  summarise(across(c(F1, F2), .fns = mean)) |>\n  ungroup() |>\n  ggplot(aes(F2, F1, color = name))+\n    geom_text(aes(label = ipa_vclass))+\n    ggforce::geom_mark_hull(aes(fill = name))+\n    scale_x_reverse()+\n    scale_y_reverse()+\n    khroma::scale_color_vibrant()+\n    khroma::scale_fill_vibrant()\n```\n\n::: {.cell-output-display}\n![Speakers' vowel means in unnormalized F1,F2 space.](index_files/figure-html/fig-unnorm1-1.png){#fig-unnorm1 fig-alt='An F1 by F2 plot of two speakers\\' unnormalized vowel means.' width=576}\n:::\n:::\n\n\nHere, we see reason number 1 why we'll want to normalize vowels. These two speakers vowel spaced hardly overlap, but the *relative* position of vowel categories inside their spaces are fairly similar. All normalization methods try to do is pinch and scale *appropriately* to get the relative positions of these vowel spaces to overlap.\n\n*Another* reason we might be motivated to normalize our data is because F2 has a much larger range of data than F1. We can more easily see that if we add `coord_fixed()` to the plot.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plot code\"}\nvowels_focus |>\n  group_by(name, ipa_vclass) |>\n  summarise(across(c(F1, F2), .fns = mean)) |>\n  ungroup() |>\n  ggplot(aes(F2, F1, color = name))+\n    geom_text(aes(label = ipa_vclass))+\n    ggforce::geom_mark_hull(aes(fill = name))+\n    scale_x_reverse()+\n    scale_y_reverse()+\n    khroma::scale_color_vibrant()+\n    khroma::scale_fill_vibrant()+\n    coord_fixed()\n```\n\n::: {.cell-output-display}\n![Speakers' vowel means in unnormalized F1,F2 space. (fixed coords)](index_files/figure-html/fig-unnorm2-1.png){#fig-unnorm2 fig-alt='An F1 by F2 plot of two speakers\\' unnormalized vowel means.' width=576}\n:::\n:::\n\n\nThe plot is squished because F2 just has that much larger a range of values than F1. Any stats or calculations we do on vowels in the F1$\\times$F2 space is going to be dominated by things that happen across F2 vs F1.\n\n## Prepping for normalization\n\nIn order to get ready for normalization, I'm going to first filter out any vowel tokens that have a $\\sqrt{\\text{mahalanobis}}$ distance from its vowel class greater than 2.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvowels_focus |>\n  group_by(name, ipa_vclass) |>\n  mutate(mahal = joeyr::tidy_mahalanobis(F1, F2),\n         mahal_sq = sqrt(mahal)) |>\n  filter(mahal_sq <= 2) |>\n  ungroup() -> vowels_inlie\n```\n:::\n\n\n## Normalizations\n\n### Lobanov a.k.a. z-score\n\nThe first normalization procedure we'll look at is \"Lobanov\" normalization [@lobanov]. An important thing to know here is that while phoneticians may call this procedure \"Lobanov\" normalization, the rest of the data analysis world calls it \"z-score\" or even just \"standard score.\"\n\nThe process works like this:\n\n-   Within each speaker's data ⤵️\n\n    -   Within each formant ⤵️\n\n        -   subtract the mean of the formant, and divide by the standard deviation of the formant\n\nSince we already a have F1 and F2 separated out into separate columns, this means all we have to do us `group_by()` our vowels data by speaker ID (in `name`), then create our new normalized columns by subtracting the mean and dividing by the standard deviation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvowels_inlie |>\n  group_by(name) |>\n  mutate(F1_z = (F1 - mean(F1))/sd(F1),\n         F2_z = (F2 - mean(F2))/sd(F2)) -> vowels_score1\n```\n:::\n\n\nThe `group_by(name)` process in this pipeline means that when we calculate `mean(F1)` and `std(F1)` in the next part, those mean and standard deviation values are for each speaker's subset of `vowels_inlie`.\n\nSince z-scoring is so common a thing to do in quantitative analysis, R actually has a built in function, `scale()` that we could use instead of writing the math out ourselves.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvowels_inlie |>\n  group_by(name) |>\n  mutate(F1_z = scale(F1),\n         F2_z = scale(F2)) -> vowels_zscore2\n```\n:::\n\n\n#### Results\n\nHere's the results of z-scoring! We've got largely overlapping vowel spaces now. The numeric values of these z-scores tend to run somewhere between -3 and 3 (less extreme for means). Both formants are also on the same scale, which you can see with the plot being roughly square shaped even with `coord_fixed()` being added.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plot code\"}\nvowels_zscore2 |>\n  group_by(name, ipa_vclass) |>\n  summarise(across(c(F1_z, F2_z), .fns = mean)) |>\n  ggplot(aes(F2_z, F1_z, color = name))+\n    geom_text(aes(label = ipa_vclass))+\n    ggforce::geom_mark_hull(aes(fill = name))+\n    scale_x_reverse()+\n    scale_y_reverse()+\n    khroma::scale_color_vibrant()+\n    khroma::scale_fill_vibrant()+\n    coord_fixed()\n```\n\n::: {.cell-output-display}\n![Speakers' vowel means in z-scored F1,F2 space. (fixed coords)](index_files/figure-html/fig-zscore-1.png){#fig-zscore fig-alt='An F1 by F2 plot of two speakers\\' Lobanov normalized vowel means.' width=576}\n:::\n:::\n\n\n### Nearey (a.k.a. Nearey 2)\n\nNext, we'll look at what the vowels package calls \"Nearey 2\" normalization [@nearey1978]. The Nearey 1 procedure is more similar in terms of data code as z-scoring.[^1] The Nearey 2 procedure works like this:\n\n[^1]:\n    ``` r\n    vowels_inlie |>\n      group_by(name) |>\n      mutate(F1_n = exp(log(F1) - mean(log(F1))),\n             F2_n = exp(log(F2) - mean(log(F2))))\n    ```\n\n-   Within each speaker ⤵️\n\n    -   Convert *all* formant measurements to log-Hz\n\n    -   Get the mean log-Hz across all measurements (F1 and F2 *together*)\n\n    -   Subtract the mean log-Hz from the log-Hz\n\n    -   \"Anti-log\" or exponentiate the result\n\nWe need to combine F1 and F2 measurements together to get their mean, and this is best achieved in the tidyverse by \"pivoting\" the data frame from wide to long. We'll take it in steps. In each next step, I'm going to be putting additional tidyverse verbs between the data frame and the line that has `head() |> rmarkdown::paged_table()`, which is just there to provide nice looking output.\n\n#### Step 1: Focus in on the columns of interest\n\nThe `vowels_inlie` has data columns for the mahalanobis distance, which I want to drop off for now.\n\n\n::: {#tbl-nearey-focus .cell tbl-cap='The columns of importance for Neary 2'}\n\n```{.r .cell-code}\nvowels_inlie |>\n  select(name, word, ipa_vclass, F1, F2) |>\n  # more verbs to go here\n  head() |> rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ipa_vclass\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"F1\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F2\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"s01\",\"2\":\"UM\",\"3\":\"ʌ\",\"4\":\"699.5\",\"5\":\"1881.3\"},{\"1\":\"s01\",\"2\":\"I'M\",\"3\":\"aj\",\"4\":\"888.8\",\"5\":\"1934.1\"},{\"1\":\"s01\",\"2\":\"IN\",\"3\":\"ɪ\",\"4\":\"612.2\",\"5\":\"2323.4\"},{\"1\":\"s01\",\"2\":\"COLUMBUS\",\"3\":\"ə\",\"4\":\"612.4\",\"5\":\"1903.7\"},{\"1\":\"s01\",\"2\":\"ENTIRE\",\"3\":\"ɪ\",\"4\":\"529.9\",\"5\":\"2332.1\"},{\"1\":\"s01\",\"2\":\"ENTIRE\",\"3\":\"ə˞\",\"4\":\"538.4\",\"5\":\"1682.8\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n#### Step 2: Add an ID column\n\nWhen we're pivoting our data long and then wide again, we need to have a column that is a unique id for each vowel observation. I'll create that with `mutate(id = 1:n())` . The `n()` function there is a convenience function that returns the number of rows in each (group) of the data frame.\n\n\n::: {#tbl-nearey-id .cell tbl-cap='The formant data with a token ID for each observation'}\n\n```{.r .cell-code}\nvowels_inlie |>\n  select(name, word, ipa_vclass, F1, F2) |>\n  mutate(id = 1:n()) |>\n  # more verbs to go here\n  head() |> rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ipa_vclass\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"F1\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F2\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"id\"],\"name\":[6],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"s01\",\"2\":\"UM\",\"3\":\"ʌ\",\"4\":\"699.5\",\"5\":\"1881.3\",\"6\":\"1\"},{\"1\":\"s01\",\"2\":\"I'M\",\"3\":\"aj\",\"4\":\"888.8\",\"5\":\"1934.1\",\"6\":\"2\"},{\"1\":\"s01\",\"2\":\"IN\",\"3\":\"ɪ\",\"4\":\"612.2\",\"5\":\"2323.4\",\"6\":\"3\"},{\"1\":\"s01\",\"2\":\"COLUMBUS\",\"3\":\"ə\",\"4\":\"612.4\",\"5\":\"1903.7\",\"6\":\"4\"},{\"1\":\"s01\",\"2\":\"ENTIRE\",\"3\":\"ɪ\",\"4\":\"529.9\",\"5\":\"2332.1\",\"6\":\"5\"},{\"1\":\"s01\",\"2\":\"ENTIRE\",\"3\":\"ə˞\",\"4\":\"538.4\",\"5\":\"1682.8\",\"6\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n#### Step 3: Pivot Longer\n\nNow, we want to take the F1 and F2 columns, and stack them one on top of each other, which we can do with `pivot_longer()`. This function needs to know at least three things:\n\n-   Which columns are we going to be stacking on top of each other?\n\n    -   We pass this information to the `cols=` argument.\n\n-   How should we keep track of the original column names?\n\n    -   We pass this information to `names_to=`.\n\n-   How should we keep track of the original values from these columns?\n\n    -   We pass this information to `values_to=`\n\n\n::: {#tbl-nearey-long .cell tbl-cap='The formant data, pivoted long'}\n\n```{.r .cell-code}\nvowels_inlie |>\n  select(name, word, ipa_vclass, F1, F2) |>\n  mutate(id = 1:n()) |>\n  pivot_longer(cols = F1:F2, \n               names_to = \"formant\", \n               values_to = \"hz\") |>  \n  # more verbs to go here\n  head(12) |> rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ipa_vclass\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"id\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"formant\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"hz\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"s01\",\"2\":\"UM\",\"3\":\"ʌ\",\"4\":\"1\",\"5\":\"F1\",\"6\":\"699.5\"},{\"1\":\"s01\",\"2\":\"UM\",\"3\":\"ʌ\",\"4\":\"1\",\"5\":\"F2\",\"6\":\"1881.3\"},{\"1\":\"s01\",\"2\":\"I'M\",\"3\":\"aj\",\"4\":\"2\",\"5\":\"F1\",\"6\":\"888.8\"},{\"1\":\"s01\",\"2\":\"I'M\",\"3\":\"aj\",\"4\":\"2\",\"5\":\"F2\",\"6\":\"1934.1\"},{\"1\":\"s01\",\"2\":\"IN\",\"3\":\"ɪ\",\"4\":\"3\",\"5\":\"F1\",\"6\":\"612.2\"},{\"1\":\"s01\",\"2\":\"IN\",\"3\":\"ɪ\",\"4\":\"3\",\"5\":\"F2\",\"6\":\"2323.4\"},{\"1\":\"s01\",\"2\":\"COLUMBUS\",\"3\":\"ə\",\"4\":\"4\",\"5\":\"F1\",\"6\":\"612.4\"},{\"1\":\"s01\",\"2\":\"COLUMBUS\",\"3\":\"ə\",\"4\":\"4\",\"5\":\"F2\",\"6\":\"1903.7\"},{\"1\":\"s01\",\"2\":\"ENTIRE\",\"3\":\"ɪ\",\"4\":\"5\",\"5\":\"F1\",\"6\":\"529.9\"},{\"1\":\"s01\",\"2\":\"ENTIRE\",\"3\":\"ɪ\",\"4\":\"5\",\"5\":\"F2\",\"6\":\"2332.1\"},{\"1\":\"s01\",\"2\":\"ENTIRE\",\"3\":\"ə˞\",\"4\":\"6\",\"5\":\"F1\",\"6\":\"538.4\"},{\"1\":\"s01\",\"2\":\"ENTIRE\",\"3\":\"ə˞\",\"4\":\"6\",\"5\":\"F2\",\"6\":\"1682.8\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nYou can get an idea for what `pivot_longer()` has done by comparing this table (@tbl-nearey-long) to the table before (@tbl-nearey-id). There are two rows in this long table for every row in the wide table. The `name`, `word`, `ipa_vclass`, and `id` values are repeated twice. There's also a new `formant` column, which contains the column names we pivoted longer, and a new `hz` column, which contains the values from the columns we pivoted longer.\n\n#### Step 4: The actual normalization!\n\nNow we can do the actual normalization. The column we'll target is the new `hz` column. We'll log it, subtract the mean log value, then convert it back to its original scale with `exp()`. And don't forget we need to `group_by(name)` first too!\n\n\n::: {#tbl-nearey-norm .cell tbl-cap='The formant data, pivoted long, and normalized'}\n\n```{.r .cell-code}\nvowels_inlie |>\n  select(name, word, ipa_vclass, F1, F2) |>\n  mutate(id = 1:n()) |>\n  pivot_longer(cols = F1:F2, \n               names_to = \"formant\", \n               values_to = \"hz\") |>\n  group_by(name) |>\n  mutate(nearey = exp(log(hz)-mean(log(hz))))|>  \n  # more verbs to go here\n  head(12) |> rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ipa_vclass\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"id\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"formant\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"hz\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"nearey\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"s01\",\"2\":\"UM\",\"3\":\"ʌ\",\"4\":\"1\",\"5\":\"F1\",\"6\":\"699.5\",\"7\":\"0.6325928\"},{\"1\":\"s01\",\"2\":\"UM\",\"3\":\"ʌ\",\"4\":\"1\",\"5\":\"F2\",\"6\":\"1881.3\",\"7\":\"1.7013537\"},{\"1\":\"s01\",\"2\":\"I'M\",\"3\":\"aj\",\"4\":\"2\",\"5\":\"F1\",\"6\":\"888.8\",\"7\":\"0.8037863\"},{\"1\":\"s01\",\"2\":\"I'M\",\"3\":\"aj\",\"4\":\"2\",\"5\":\"F2\",\"6\":\"1934.1\",\"7\":\"1.7491034\"},{\"1\":\"s01\",\"2\":\"IN\",\"3\":\"ɪ\",\"4\":\"3\",\"5\":\"F1\",\"6\":\"612.2\",\"7\":\"0.5536431\"},{\"1\":\"s01\",\"2\":\"IN\",\"3\":\"ɪ\",\"4\":\"3\",\"5\":\"F2\",\"6\":\"2323.4\",\"7\":\"2.1011669\"},{\"1\":\"s01\",\"2\":\"COLUMBUS\",\"3\":\"ə\",\"4\":\"4\",\"5\":\"F1\",\"6\":\"612.4\",\"7\":\"0.5538240\"},{\"1\":\"s01\",\"2\":\"COLUMBUS\",\"3\":\"ə\",\"4\":\"4\",\"5\":\"F2\",\"6\":\"1903.7\",\"7\":\"1.7216112\"},{\"1\":\"s01\",\"2\":\"ENTIRE\",\"3\":\"ɪ\",\"4\":\"5\",\"5\":\"F1\",\"6\":\"529.9\",\"7\":\"0.4792151\"},{\"1\":\"s01\",\"2\":\"ENTIRE\",\"3\":\"ɪ\",\"4\":\"5\",\"5\":\"F2\",\"6\":\"2332.1\",\"7\":\"2.1090347\"},{\"1\":\"s01\",\"2\":\"ENTIRE\",\"3\":\"ə˞\",\"4\":\"6\",\"5\":\"F1\",\"6\":\"538.4\",\"7\":\"0.4869021\"},{\"1\":\"s01\",\"2\":\"ENTIRE\",\"3\":\"ə˞\",\"4\":\"6\",\"5\":\"F2\",\"6\":\"1682.8\",\"7\":\"1.5218402\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n#### Step 5: Pivoting wide again, for plotting\n\nTechnically, we're done normalizing the formant data, but in order to make a nice F1$\\times$F2 plot, we need to pivot the data wide again.\n\nTo do that, we first need to drop the `hz` column, because `pivot_wider()` will go weird if we don't.[^2] Then, we need to tell `pivot_wider()` the following information:\n\n[^2]: But you should try it just to see!\n\n-   Where should it get the names of new columns from?\n\n    -   We'll pass this to `names_from=`\n\n-   Where should it get the values to put into the new columns from?\n\n    -   We'll pass this to `values_from=`\n\nAfter this step, we're done the normalizing process, so I'll assign the result to a variable called `vowels_neary`\n\n\n::: {#tbl-neary-wide .cell tbl-cap='Nearey Normalized Vowels, wide again'}\n\n```{.r .cell-code}\nvowels_inlie |>\n  select(name, word, ipa_vclass, F1, F2) |>\n  mutate(id = 1:n()) |>\n  pivot_longer(cols = F1:F2, \n               names_to = \"formant\", \n               values_to = \"hz\") |>\n  group_by(name) |>\n  mutate(nearey = exp(log(hz)-mean(log(hz))))|>  \n  select(-hz) |>\n  pivot_wider(names_from = formant, values_from = nearey) -> vowels_nearey\n\nvowels_nearey |>\n  head() |> rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ipa_vclass\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"id\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"F1\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F2\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"s01\",\"2\":\"UM\",\"3\":\"ʌ\",\"4\":\"1\",\"5\":\"0.6325928\",\"6\":\"1.701354\"},{\"1\":\"s01\",\"2\":\"I'M\",\"3\":\"aj\",\"4\":\"2\",\"5\":\"0.8037863\",\"6\":\"1.749103\"},{\"1\":\"s01\",\"2\":\"IN\",\"3\":\"ɪ\",\"4\":\"3\",\"5\":\"0.5536431\",\"6\":\"2.101167\"},{\"1\":\"s01\",\"2\":\"COLUMBUS\",\"3\":\"ə\",\"4\":\"4\",\"5\":\"0.5538240\",\"6\":\"1.721611\"},{\"1\":\"s01\",\"2\":\"ENTIRE\",\"3\":\"ɪ\",\"4\":\"5\",\"5\":\"0.4792151\",\"6\":\"2.109035\"},{\"1\":\"s01\",\"2\":\"ENTIRE\",\"3\":\"ə˞\",\"4\":\"6\",\"5\":\"0.4869021\",\"6\":\"1.521840\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n#### Results!\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plot code\"}\nvowels_nearey |>\n  group_by(name, ipa_vclass) |>\n  summarise(across(c(F1, F2), .fns = mean)) |>\n  ggplot(aes(F2, F1, color = name))+\n    geom_text(aes(label = ipa_vclass))+\n    ggforce::geom_mark_hull(aes(fill = name))+\n    scale_x_reverse()+\n    scale_y_reverse()+\n    khroma::scale_color_vibrant()+\n    khroma::scale_fill_vibrant()\n```\n\n::: {.cell-output-display}\n![An F1 by F2 plot of two speakers' Nearey normalized vowel means.](index_files/figure-html/fig-neary-norm-1.png){#fig-neary-norm width=624}\n:::\n:::\n\n\nAgain, the vowel spaces are largely overlapping. One thing that's not immediately clear from this graph, though, is that this specific approach to Nearey normalization still has a much wider range for F2 than F1, which we can see if we plot it with fixed coordinates.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plot code\"}\nvowels_nearey |>\n  group_by(name, ipa_vclass) |>\n  summarise(across(c(F1, F2), .fns = mean)) |>\n  ggplot(aes(F2, F1, color = name))+\n    geom_text(aes(label = ipa_vclass))+\n    ggforce::geom_mark_hull(aes(fill = name))+\n    scale_x_reverse()+\n    scale_y_reverse()+\n    khroma::scale_color_vibrant()+\n    khroma::scale_fill_vibrant()+\n    coord_fixed()\n```\n\n::: {.cell-output-display}\n![An F1 by F2 plot of two speakers' Nearey normalized vowel means. (fixed coordinates).](index_files/figure-html/fig-neary-norm-fixed-1.png){#fig-neary-norm-fixed width=624}\n:::\n:::\n\n\nIt gets better if we plot the log of the normalized values[^3], but then the vowel space doesn't look like the familiar hertz space.\n\n[^3]: Which is a little silly because that's what we had in the normalization code chunk above, and then we did `exp()` on it.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plot code\"}\nvowels_nearey |>\n  group_by(name, ipa_vclass) |>\n  summarise(across(c(F1, F2), .fns = ~mean(log(.x)))) |>\n  ggplot(aes(F2, F1, color = name))+\n    geom_text(aes(label = ipa_vclass))+\n    ggforce::geom_mark_hull(aes(fill = name))+\n    scale_x_reverse()+\n    scale_y_reverse()+\n    khroma::scale_color_vibrant()+\n    khroma::scale_fill_vibrant()+\n    coord_fixed()\n```\n\n::: {.cell-output-display}\n![Speakers' vowel means in Log Nearey Normalized F1,F2 space (fixed coordinates).](index_files/figure-html/fig-neary-log-norm-fixed-1.png){#fig-neary-log-norm-fixed width=624}\n:::\n:::\n\n\n### Watt & Fabricius\n\nThe final normalization method we'll look at, which can't easily be done with a single sequence of tidyverse verbs, is the Watt & Fabricius method [@watt2002]. The idea behind the Watt & Fabricius method is to define a vowel space triangle for each speaker, then to scale their formant values to the triangle.\n\n#### Step 1: Getting the Triangle Points\n\nI'll follow the NORM suite and define the points of the triangle like so:\n\n-   Top Left Corner, \"beet\" point: The F1 of the vowel with the smallest F1, and the F2 of the vowel with the largest F2\n\n-   Bottom Corner, \"bat\" point: The F1 of the vowel with the maximum F1, and the F2 of that same vowel.\n\n-   Top Right Corner, \"school\" point: F1 = F2 = \"beet\" point F1.\n\nHere's how you can get those values per speaker:\n\n\n::: {#tbl-wf-points .cell tbl-cap='The vowel triangle point values.'}\n\n```{.r .cell-code}\nvowels_inlie |>\n  group_by(name, ipa_vclass) |>\n  summarise(across(c(F1, F2), mean)) |>\n  group_by(name) |>\n  summarise(beet_f1 = min(F1),\n            beet_f2 = max(F2),\n            bat_f1 = max(F1),\n            bat_f2 = F2[F1 == max(F1)]) |>\n  mutate(school_f1 = beet_f1,\n         school_f2 = beet_f1) |>\n  # more verbs to go here\n rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"beet_f1\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"beet_f2\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"bat_f1\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"bat_f2\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"school_f1\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"school_f2\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"s01\",\"2\":\"430.4145\",\"3\":\"2595.765\",\"4\":\"819.6912\",\"5\":\"1901.572\",\"6\":\"430.4145\",\"7\":\"430.4145\"},{\"1\":\"s03\",\"2\":\"316.1036\",\"3\":\"1989.806\",\"4\":\"674.2896\",\"5\":\"1242.969\",\"6\":\"316.1036\",\"7\":\"316.1036\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nIn order to plot these points in an F1$\\times$F2 space, we need to do a little `pivot_longer()` and `pivot_wider()` again with the following steps:\n\n-   `pivot_longer()`, to stack the values of the columns between `beet_f1` and `school_f2` on top of each other.\n\n-   Split the point names (e.g. \"`beet_f1\"`) into separate values (e.g. `\"beet\"` and `\"beet\"`) using `separate()`.\n\n-   `pivot_wider()` using the column with `\"f1\"` and `\"f2\"` to make the new column names.\n\n\n::: {#tbl-wf-point-long .cell tbl-cap='The vowel triangle point values, long version'}\n\n```{.r .cell-code}\nvowels_inlie |>\n  group_by(name, ipa_vclass) |>\n  summarise(across(c(F1, F2), mean)) |>\n  group_by(name) |>\n  summarise(beet_f1 = min(F1),\n            beet_f2 = max(F2),\n            bat_f1 = max(F1),\n            bat_f2 = F2[F1 == max(F1)]) |>\n  mutate(school_f1 = beet_f1,\n         school_f2 = beet_f1) |>\n  pivot_longer(cols = beet_f1:school_f2, names_to = \"var\", values_to = \"hz\") |>\n  separate(var, into = c(\"point\", \"formant\")) |>\n  pivot_wider(names_from = formant, values_from = hz) -> wf_points\n\nwf_points |>\n  rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"point\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"f1\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"f2\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"s01\",\"2\":\"beet\",\"3\":\"430.4145\",\"4\":\"2595.7652\"},{\"1\":\"s01\",\"2\":\"bat\",\"3\":\"819.6912\",\"4\":\"1901.5715\"},{\"1\":\"s01\",\"2\":\"school\",\"3\":\"430.4145\",\"4\":\"430.4145\"},{\"1\":\"s03\",\"2\":\"beet\",\"3\":\"316.1036\",\"4\":\"1989.8057\"},{\"1\":\"s03\",\"2\":\"bat\",\"3\":\"674.2896\",\"4\":\"1242.9689\"},{\"1\":\"s03\",\"2\":\"school\",\"3\":\"316.1036\",\"4\":\"316.1036\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWhen we plot thee triangle points, we wind up with a plot that looks like a very simplified version of the vowel space from @fig-unnorm1.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plot code\"}\nwf_points |>\n  ggplot(aes(f2, f1))+\n    geom_polygon(aes(fill = name, color = name), alpha = 0.6)+\n    geom_text(aes(label = point))+\n    scale_x_reverse()+\n    scale_y_reverse()+\n    scale_fill_vibrant()+\n    scale_color_vibrant()\n```\n\n::: {.cell-output-display}\n![The vowel space triangles for the Watt & Fabricius method](index_files/figure-html/fig-wf-triangle-1.png){#fig-wf-triangle width=576}\n:::\n:::\n\n\n#### Step 2: Calculate Scaling Factors\n\nWith the F1 and F2 of these triangle points, we then calculate scaling factors by just taking the mean of F1 and F2 for each speaker.\n\n\n::: {#tbl-wf-scaler .cell tbl-cap='The Watt & Fabricius scaling factors'}\n\n```{.r .cell-code}\nwf_points |>\n  group_by(name) |>\n  summarise(S1 = mean(f1),\n           S2 = mean(f2)) -> wf_scalers\n\nwf_scalers |>\n rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"S1\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"S2\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"s01\",\"2\":\"560.1734\",\"3\":\"1642.584\"},{\"1\":\"s03\",\"2\":\"435.4989\",\"3\":\"1182.959\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n#### Step 3: Merging the scalers onto the original data.\n\nNow what we need to do is divide each speaker's F1 and F2 value by these scalers. Right now the speakers' original data is in `vowels_inlie` and the scaling values are in `wf_scalers`, so to do that division, we need to associate these two data frames.\n\nFortunately, both `vowels_inlie` and `wf_scalers` have a column name in common: the speaker ID column `name`. That means we can merge `wf_scalers` onto `vowels_inlie` with a join operation. There are a few different `*_join()` functions in the tidyverse, and here I'll use `left_join()`.\n\nWe need to tell `left_join()` the following information:\n\n-   Which two data frames are we joining together?\n\n    -   Since we're piping, the first data frame will appear on the left hand side of `|>`, and the second data frame will be the first argument.\n\n-   Which columns should we use to join the data frames together?\n\n    -   `left_join()` will guess and try to join the data together using columns with the same name, but we can explicitly tell it which columns to use with the `by=` argument.\n\n\n::: {#tbl-wf-join .cell tbl-cap='Speakers\\'s scalers merged onto the original data'}\n\n```{.r .cell-code}\nvowels_inlie |>\n  left_join(wf_scalers, by = \"name\") |>\n  # more verbs to go here\n  # The group_by here is just to illustrate that each speaker's scalers were\n  # successfully merged\n  group_by(name) |> slice (1:3) |> rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ipa_vclass\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"F1\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F2\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mahal\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mahal_sq\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"S1\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"S2\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"s01\",\"2\":\"UM\",\"3\":\"ʌ\",\"4\":\"699.5\",\"5\":\"1881.3\",\"6\":\"1.1248704\",\"7\":\"1.0605991\",\"8\":\"560.1734\",\"9\":\"1642.584\"},{\"1\":\"s01\",\"2\":\"I'M\",\"3\":\"aj\",\"4\":\"888.8\",\"5\":\"1934.1\",\"6\":\"0.4752467\",\"7\":\"0.6893814\",\"8\":\"560.1734\",\"9\":\"1642.584\"},{\"1\":\"s01\",\"2\":\"IN\",\"3\":\"ɪ\",\"4\":\"612.2\",\"5\":\"2323.4\",\"6\":\"1.5749328\",\"7\":\"1.2549633\",\"8\":\"560.1734\",\"9\":\"1642.584\"},{\"1\":\"s03\",\"2\":\"I\",\"3\":\"aj\",\"4\":\"771.5\",\"5\":\"1257.4\",\"6\":\"3.2346829\",\"7\":\"1.7985224\",\"8\":\"435.4989\",\"9\":\"1182.959\"},{\"1\":\"s03\",\"2\":\"HAVE\",\"3\":\"æ\",\"4\":\"639.5\",\"5\":\"1609.3\",\"6\":\"0.3964394\",\"7\":\"0.6296344\",\"8\":\"435.4989\",\"9\":\"1182.959\"},{\"1\":\"s03\",\"2\":\"A\",\"3\":\"ə\",\"4\":\"501.2\",\"5\":\"1202.0\",\"6\":\"2.0544758\",\"7\":\"1.4333443\",\"8\":\"435.4989\",\"9\":\"1182.959\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n#### Step 4: The actual normalization!\n\nNow all we need to do is divide F1 and F2 by their respective scaling factors to get the normalized Watt & Fabricius values.\n\n\n::: {#tbl-wf-norm .cell tbl-cap='Speakers\\' vowels Watt & Fabricius normalized'}\n\n```{.r .cell-code}\nvowels_inlie |>\n  left_join(wf_scalers, by = \"name\") |>\n  mutate(F1_wf = F1/S1,\n         F2_wf = F2/S2)->vowels_wf\n\nvowels_wf |>\n  group_by(name) |> slice(1:3) |> rmarkdown::paged_table()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"ipa_vclass\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"F1\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F2\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mahal\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mahal_sq\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"S1\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"S2\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F1_wf\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F2_wf\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"s01\",\"2\":\"UM\",\"3\":\"ʌ\",\"4\":\"699.5\",\"5\":\"1881.3\",\"6\":\"1.1248704\",\"7\":\"1.0605991\",\"8\":\"560.1734\",\"9\":\"1642.584\",\"10\":\"1.248720\",\"11\":\"1.145330\"},{\"1\":\"s01\",\"2\":\"I'M\",\"3\":\"aj\",\"4\":\"888.8\",\"5\":\"1934.1\",\"6\":\"0.4752467\",\"7\":\"0.6893814\",\"8\":\"560.1734\",\"9\":\"1642.584\",\"10\":\"1.586651\",\"11\":\"1.177474\"},{\"1\":\"s01\",\"2\":\"IN\",\"3\":\"ɪ\",\"4\":\"612.2\",\"5\":\"2323.4\",\"6\":\"1.5749328\",\"7\":\"1.2549633\",\"8\":\"560.1734\",\"9\":\"1642.584\",\"10\":\"1.092876\",\"11\":\"1.414479\"},{\"1\":\"s03\",\"2\":\"I\",\"3\":\"aj\",\"4\":\"771.5\",\"5\":\"1257.4\",\"6\":\"3.2346829\",\"7\":\"1.7985224\",\"8\":\"435.4989\",\"9\":\"1182.959\",\"10\":\"1.771531\",\"11\":\"1.062927\"},{\"1\":\"s03\",\"2\":\"HAVE\",\"3\":\"æ\",\"4\":\"639.5\",\"5\":\"1609.3\",\"6\":\"0.3964394\",\"7\":\"0.6296344\",\"8\":\"435.4989\",\"9\":\"1182.959\",\"10\":\"1.468431\",\"11\":\"1.360402\"},{\"1\":\"s03\",\"2\":\"A\",\"3\":\"ə\",\"4\":\"501.2\",\"5\":\"1202.0\",\"6\":\"2.0544758\",\"7\":\"1.4333443\",\"8\":\"435.4989\",\"9\":\"1182.959\",\"10\":\"1.150864\",\"11\":\"1.016096\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n#### Results!\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plot code\"}\nvowels_wf |>\n  group_by(name, ipa_vclass) |>\n  summarise(across(c(F1_wf, F2_wf), .fns = mean)) |>\n  ggplot(aes(F2_wf, F1_wf, color = name))+\n    geom_text(aes(label = ipa_vclass))+\n    ggforce::geom_mark_hull(aes(fill = name))+\n    scale_x_reverse()+\n    scale_y_reverse()+\n    scale_color_vibrant()+\n    scale_fill_vibrant()+\n    coord_fixed()\n```\n\n::: {.cell-output-display}\n![An F1 by F2 plot of two speakers' Watt & Fabricius normalized vowel means.](index_files/figure-html/fig-wf-norm-1.png){#fig-wf-norm width=576}\n:::\n:::\n\n\nAgain, we've got largely overlapping vowel spaces, and since F1 and F2 were normalized separately, they've got very comparable data ranges.\n\n## Final thoughts\n\nThere there has been a *lot* written about vowel normalization methods, and I've barely covered most of the relevant topics discussed in the literature. For example:\n\n-   How well do these methods correspond to what *listeners* actually do when hearing people with different vowel spaces?\n\n-   Should we really be plotting and analyzing formants in Hz, or should the by converted to a psychoacoustic dimension, like Mel or Bark?\n\n-   Which method is the \"Best\"? What do we want a method to be \"Best\" *at*?\n\nInstead, I've covered some of most commonly used normalization methods, and also tried to provide an outline of how these methods correspond to specific data frame operations in the tidyverse.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}