{
  "hash": "928d0f065c4a6f53fd12d1ae43fe86c0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting VOT-related landmarks in R with `getVOT`\"\nauthor:\n  - name: \n      given: \"Rasmus\"\n      family: \"Puggaard-Rode\"\n    url: \"http://rasmuspuggaard.wordpress.com\"\n    affiliations:\n      - name: \"Institute for Phonetics and Speech Processing, Ludwig Maximilian University of Munich\"\ndate: 2023-06-14\nlicense: \"CC-BY-SA 4.0\"\nformat: \n  html: default\n  pdf: default\neditor: visual\nknitr: \n  opts_chunk: \n    message: false\nbibliography: references.bib\n---\n\n## Overview\n\n`getVOT` is a developmental R package which predicts voice onset time (VOT) automatically. It can be installed from GitHub like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install devtools if needed\n#install.packages('devtools')\nlibrary(devtools)\ndevtools::install_github('rpuggaardrode/getVOT')\n```\n:::\n\n\nIn most cases, VOT can be delimited visually from a waveform using a few clearly defined landmarks.\n\nIn voiceless stops, the first landmark is a sudden increase in amplitude following a period of silence, which corresponds to the stop release, and the second landmark is the onset of periodicity in the waveform corresponding to the onset of voicing. These two landmarks are shown here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#load sound file and TextGrid using rPraat\nvl_snd <- rPraat::snd.read('vl/1.wav')\nvl_tg <- rPraat::tg.read('vl/1.TextGrid')\n\n#rPraat loads sound files as a list, containing e.g. a vector of audio samples ($sig)\n#which we plot as a regular line plot with the base plotting function.\n#the list also has information about time stamps ($t) which we plot on the x axis,\n#showing 50 ms to 250 ms.\nplot(x=vl_snd$t, y=vl_snd$sig, type='l', xlab='Time (s)', ylab='Amplitude', \n     xlim=c(0.05, 0.25))\n#TextGrids are loaded as a named list of tiers, this one has a tier called 'man'\n#containing manually annotated VOT.\n#each tier is itself a list including information about segmentations and start\n#and end time of each interval.\n#here we plot a blue vertical line for each interval of 'man'\nabline(v=vl_tg$man$t1, col='blue')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nIn voiced stops, the process is essentially reversed: the first landmark is the onset of periodicity following a period of silence corresponding to the voicing onset, and the second landmark is a sudden increase in amplitude corresponding to the stop release. (Alternatively, the stop release is signaled by a *transient phase*, which is not easy to spot in the waveform but can usually be recognized by its spectral shape.) These two landmarks are shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvd_snd <- rPraat::snd.read('vd/1.wav')\nvd_tg <- rPraat::tg.read('vd/1.TextGrid')\n\nplot(x=vd_snd$t, y=vd_snd$sig, type='l', xlab='Time (s)', ylab='Amplitude', \n     xlim=c(0.05, 0.25))\nabline(v=vd_tg$man$t1, col='blue')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n`getVOT` attempts to predict these landmarks by loading sound files into R and using relatively simple algorithms to find those landmarks in the vector of samples corresponding to the waveform.\n\nThe goal is to partially make automatic, or at least aid in, the job of annotating VOT. There are other and more sophisticated algorithms available for predicting VOT, including [AutoVOT](https://github.com/mlml/autovot) (see [Eleanor Chodroff's tutorial](https://lingmethodshub.github.io/content/tools/autovot/autovot-tutorial/)) and [Dr.VOT](https://github.com/MLSpeech/Dr.VOT). The job of `getVOT` is not to replace or even necessarily outperform these. AutoVOT only predicts positive VOT and scales best with a fair amount of training data; Dr.VOT predicts both positive and negative VOT, but can be a bit daunting to set up and difficult to manipulate; both can only be used on Unix-style operating systems. `getVOT` should be easy to set up and get started with, should be compatible with all operating systems, and should scale fairly well with little to no training data. This makes `getVOT` optimal for smaller data sets where providing suitable training data is difficult.\n\nIt should be noted though that this tutorial presents the very first publicly available version of `getVOT`, which is so far available only on GitHub. It is not perfect and the plan is to keep improving the code If you as a user run into any issues, you are encouraged to report an issue on the [GitHub page](https://github.com/rpuggaardrode/getVOT/issues).\n\nIn the next sections of this tutorial I give some examples of the most common use cases of the package. After that, for those interested, I summarize what is happening under the hood. I hope this may serve as general inspiration for those who want to solve acoustic problems in R programmatically. Perhaps inspiration will strike, and you'll even come up with a more suitable method for finding VOT-related landmarks!\n\n## Predicting VOT with default parameters\n\nThe two most important `getVOT` functions in daily use are `VOT2newTG()` and `addVOT2TG()`. As the names suggest, `VOT2newTG()` will generate a new TextGrid where no annotations exist already. In this case, sound files should be short and consist of only a single word beginning with a stop. `addVOT2TG()` adds a new tier to an existing TextGrid with predicted VOT. In this case sound files can be any duration, but the existing TextGrid should indicate the rough location of stops.\n\n### `VOT2newTG()`\n\n`VOT2newTG()` works like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(getVOT)\nVOT2newTG(directory='vl', sign='positive')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"1.TextGrid was not processed as this function only works with\\n                   sound files with the .wav extension\"\n[1] \"1.wav processed\"\n```\n\n\n:::\n:::\n\n\nThe `directory` argument takes the name of a directory with sound files to be processed. The `sign` argument tells `getVOT` to use either positive or negative VOT. The default is actually both, i.e. `sign = c('positive', 'negative')`; in this case, a simple algorithm is used to try to predict whether VOT is positive or negative, but this algorithm is far from perfect so for now it is recommended to specify either `positive` or `negative`.\n\nAfter running the above code, a new subdirectory of `vl` is created called `tg`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist.dirs('vl', recursive=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"vl\"    \"vl/tg\"\n```\n\n\n:::\n:::\n\n\nThe `tg` subdirectory contains TextGrid files for each processed sound file (which in this toy case is just 1):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist.files('vl/tg')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"1.TextGrid\"\n```\n\n\n:::\n:::\n\n\nLet's load that TextGrid into R with `rPraat::tg.read()` and show the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvl_tg_predVOT <- rPraat::tg.read('vl/tg/1.TextGrid')\nplot(x=vl_snd$t, y=vl_snd$sig, type='l', xlab='Time (s)', ylab='Amplitude')\n#first element of the t1 vector excluded because 0 is always the start time of an interval\nabline(v=vl_tg_predVOT$vot$t1[-1], col='blue')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThe results are a quite good, but a few ms off -- we will return to this point in the next section.\n\n### `addVOT2TG()`\n\n`addVOT2TG()` works like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\naddVOT2TG(directory='vd', sign='negative', \n          tg_tier='stops', seg_list='b')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"1 processed\"\n```\n\n\n:::\n:::\n\n\nThe `directory` must be the name of a directory with sound file/TextGrid pairs, indicating roughly where to look for stops. In this case, we have just a single pair where the TextGrid has a tier named `stops` (the `tg_tier` argument), with one interval labeled `b` (the `seg_list` argument). This does not have to be a perfect match; the function will search for all intervals where the first character matches the strings in `seg_list`; this can be a single string like `'b'`, but could also be multiple strings like `c('b', 'd', 'g')`.\n\nThis was the position of the interval in the original TextGrid:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x=vd_snd$t, y=vd_snd$sig, type='l', xlab='Time (s)', ylab='Amplitude')\nabline(v=vd_tg$stops$t1[-1], col='red')\n#write the TextGrid label at specified coordinates\ntext(y=-0.3, x=0.11, labels=vd_tg$stops$label, col='blue', cex=2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nAs before, the function creates a new subdirectory `vd/vot` with copies of the original TextGrids and a new tier with predicted VOT. The name of the new tier is set with the `addVOT2TG()` argument `new_tier_name`; default is `vot`. Let's load in the new TextGrid and have a look at the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvd_tg_predVOT <- rPraat::tg.read('vd/vot/1.TextGrid')\nplot(x=vd_snd$t, y=vd_snd$sig, type='l', xlab='Time (s)', ylab='Amplitude')\nabline(v=vd_tg$stops$t1[-1], col='red')\ntext(y=-0.3, x=0.11, labels=vd_tg$stops$label, col='blue', cex=2)\nabline(v=vd_tg_predVOT$vot$t1[-1], col='blue')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n## Setting parameters with training data\n\nUnder the hood, the functions we've just seen will call the functions `positiveVOT()` and `negativeVOT()` which is where the actual VOT prediction is implemented. These functions take a bunch of obscure arguments which can be used to tweak the performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nargs(positiveVOT)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (sound, sr, closure_interval = 10, release_param = 15, \n    rel_method = \"diff\", vo_method = \"acf\", vo_granularity = 1, \n    vo_param = 0.85, f0_wl = 30, f0_minacf = 0.5, zcr_min = 0.05, \n    soe_min = 0.01, burst_only = FALSE, vo_only = FALSE, rel_offset = 0, \n    f0_first = FALSE, plot = TRUE, params_list = NULL) \nNULL\n```\n\n\n:::\n\n```{.r .cell-code}\nargs(negativeVOT)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (sound, sr, vo_method = \"soe\", closure_interval = 10, \n    vo_granularity = 1.2, vo_param = 0.9, f0_wl = 50, f0_minacf = 0.5, \n    zcr_min = 0.02, soe_min = 0.01, vo_only = FALSE, rel_method = \"amplitude\", \n    release_param = 15, plot = TRUE, params_list = NULL) \nNULL\n```\n\n\n:::\n:::\n\n\nThese parameters cannot be changed directly in `VOT2newTG()` and `addVOT2TG()`, but both functions take the arguments `pos_params_list` and `neg_params_list` which can be used to change the parameters. These `params_list`s should be named lists containing new settings for most of the `positiveVOT()` and `negativeVOT()` arguments; they can be written by hand, but since the arguments are not very easy to interpret, the idea is that they are written automatically using training data. The default parameters are chosen because they usually give quite good results, but mileage may vary a lot, so it is recommended to do this.\n\nTraining data should consist of sound file--TextGrid pairs. The sound files should be representative of the data that the user wants processed; if there are existing TextGrids (i.e., if the target function is `addVOT2TG()`), the training data should consist of short extracted sound snippets corresponding to the existing segmentation. The TextGrid should contain manually annotated VOT in the first tier. A small training set of say 5--10 pairs should give quite good results.\n\nAutomatic parameter setting is implemented in the functions `neg_setParams()` and `pos_setParams()`. I'll demonstrate `pos_setParams()` below for a training set with three WAV--TextGrid pairs. The only obligatory argument in either case is `directory`, the location fo the training data. Note that these functions can take a long time to run -- they try to predict VOT using a large range of parameters and return a list with the parameters that minimize the average difference between predicted VOT and manually annotated VOT.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopt_params <- pos_setParams(directory='vl_training')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Testing burst detection parameter settings for 1.wav\"\n[1] \"Testing burst detection parameter settings for 2.wav\"\n[1] \"Testing burst detection parameter settings for 3.wav\"\n[1] \"On average, the selected burst detection parameters after a first pass agree with the training data within a margin of 0.014 ms\"\n[1] \"Finetuning burst detection parameter settings for 1.wav\"\n[1] \"Finetuning burst detection parameter settings for 2.wav\"\n[1] \"Finetuning burst detection parameter settings for 3.wav\"\n[1] \"On average, the selected burst detection parameters after finetuning agree with the training data within a margin of 0.014 ms\"\n[1] \"Testing voicing onset parameter settings for 1.wav\"\n[1] \"Testing voicing onset parameter settings for 2.wav\"\n[1] \"Testing voicing onset parameter settings for 3.wav\"\n[1] \"On average, the selected voicing onset parameters after a first pass agree with the training data within a margin of 2.754 ms\"\n[1] \"Finetuning voicing onset parameter settings for 1.wav\"\n[1] \"Finetuning voicing onset parameter settings for 2.wav\"\n[1] \"Finetuning voicing onset parameter settings for 3.wav\"\n[1] \"On average, the selected voicing onset parameters after finetuning agree with the training data within a margin of 1.528 ms\"\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nIn addition to returning a list of optimized parameters, the `setParams()` functions will also print a bunch of messages in the terminal and return a plot. The messages will give an indication of how well the selected parameters fit the training data; in this case, the predicted bursts are extremely close to the annotated bursts, and predicted voicing onset is on average within roughly 3 ms of annotated voicing onset, which is within a reasonable margin of error. The returned plot will show the sound wave, with blue lines indicating manually annotated VOT, and red lines indicating predicted VOT. When the function is really successful, the red lines will not be visible at all.\n\nThese are the returned parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopt_params\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$closure_interval\n[1] 8\n\n$vo_granularity\n[1] NA\n\n$vo_param\n[1] NA\n\n$rel_method\n[1] \"diff\"\n\n$release_param\n[1] 9\n\n$f0_first\n[1] FALSE\n\n$vo_method\n[1] \"zcr\"\n\n$f0_wl\nNULL\n\n$f0_minacf\nNULL\n\n$zcr_min\n[1] 0.03\n\n$soe_min\n[1] NA\n```\n\n\n:::\n:::\n\n\nNote that they differ slightly from the default `positiveVOT()` parameters we saw above. In the next section, I'll explain these parameters in more detail.\n\nWhen we previously attempted to predict the VOT of an aspirated stop, the results were a tad off. Let's try again with our `opt_params` and see if the results are better.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nVOT2newTG(directory='vl', sign='positive', pos_params_list=opt_params)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"1.TextGrid was not processed as this function only works with\\n                   sound files with the .wav extension\"\n[1] \"1.wav processed\"\n```\n\n\n:::\n:::\n\n\nWe'll load the resulting TextGrid into R, zoom into the sound a bit, and compare the previous results with default parameters (in red) with the new results with optimized parameters (in blue).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvl_tg_predVOT_opt <- rPraat::tg.read('vl/tg/1.TextGrid')\nplot(x=vl_snd$t, y=vl_snd$sig, type='l', xlab='Time (s)', ylab='Amplitude', \n     xlim=c(0.05, 0.25))\nabline(v=vl_tg_predVOT$vot$t1, col='red')\nabline(v=vl_tg_predVOT_opt$vot$t1, col='blue')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThere is no noticeable difference in the predicted burst location, but the predicted voicing onset location is significantly improved.\n\n\n::: {.cell}\n\n:::\n\n\n## How does it work?\n\nIn this section, I briefly describe what is going on under the hood when VOT is predicted. This should also help understand all the obscure arguments that make up the parameters lists.\n\n### Positive VOT\n\nPositive VOT is predicted using the function `positiveVOT()`. The process will be exemplified using this sound file:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvl_ex <- rPraat::snd.read('vl_training/1.wav')\nplot(x=vl_ex$t, y=vl_ex$sig, type='l', xlab='Time (s)', ylab='Amplitude')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nThe first step to this process is to look for a stop closure. We do this by dividing up the first half of the sound file into intervals of duration `closure_interval` (default is `10` ms), and finding the one with the lowest mean amplitude. The intervals look like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#extract the sound samples, specify column 1 to get vector instead of array (there's only 1 column so doesn't matter)\nsound <- vl_ex$sig[,1]\n#extract sample rate\nsr <- vl_ex$fs\n#set closure interval to default 10\nclosure_interval <- 10\n#convert closure interval to seconds\nci <- closure_interval/1000\n#how many 10 ms intervals in the first half of the sound file? round up\nsqlen <- ceiling(length(sound)/2 / sr / ci)\n#make vector of 10 ms intervals\nsq_clo <- seq(from=sr*ci, to=sqlen*sr*ci, by=sr*ci)\n\nplot(y=sound, x=vl_ex$t, type='l', ylab='Amplitude', xlab='Time (s)')\n#add grey vertical lines each 10 ms\nabline(v=sq_clo/sr, col='grey')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nThe midpoint of the interval with the lowest mean amplitude is assumed to be part of the stop closure. This point is marked in red below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#make empty vector\nmean_amp <- c()\n#set counter\ni <- 1\n#loop through intervals\nfor (s in sq_clo) {\n  #fill vector with mean amplitude by interval ignoring sign\n  mean_amp[i] <- mean(abs(sound[(s-(sr*ci)+1):s]))\n  #increase counter\n  i <- i+1\n}\n\n#closure is halfway through the interval with lowest mean amplitude\nclo <- (which(mean_amp==min(mean_amp)) - 0.5) * sr * ci\n\nplot(y=sound, x=vl_ex$t, type='l', ylab='Amplitude', xlab='Time (s)')\nabline(v=sq_clo/sr, col='grey')\n#add vertical line at predicted closure\nabline(v=clo/sr, col='red', lwd=2.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nNext up, we predict that the stop release is the first brief interval after the closure where the amplitude is above some baseline. We split up the rest of the sound file after the predicted closure into intervals of 1 ms. We predict that the release is the first interval where the maximum amplitude is above a 1 / `release_param` (default is `15`) proportion of the highest amplitude in the sound. The highest amplitude is marked in blue here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(y=sound, x=vl_ex$t, type='l', ylab='Amplitude', xlab='Time (s)')\n#add vertical line at highest amplitude\nabline(v=which.max(sound)/sr, col='blue', lwd=2.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nAnd this is the first interval where the maximum amplitude is above 1/15 of that, marked in red:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set release_param argument at default 15\nrelease_param <- 15\n\n#named object for number of samples per 1 ms, must be a whole number\nstep <- round(1 * (sr/1000))\n#make vector of 1ms intervals after predicted closure\nsq_rel <- seq(from=clo, to=length(sound), by=step)\n\n#empty vector, reset counter\nmax_amp <- c()\ni <- 1\n#loop through 1ms intervals\nfor (s in abs(sq_rel)) {\n  #fill vector with most extreme amplitude per interval, ignoring sign\n  max_amp[i] <- max(abs(sound[s:(s+step-1)]))\n  i <- i+1\n}\n\n#determine baseline amplitude for predicting stop release\nspike_size <- max(abs(sound))/release_param\n#make vector of intervals above baseline\nspike <- which(max_amp > spike_size)\n#determine location of first interval above baseline\nrel <- abs((clo[1] + ((spike[1])*step))-step)\n\nplot(y=sound, x=vl_ex$t, type='l', ylab='Amplitude', xlab='Time (s)')\nabline(v=which.max(sound)/sr, col='blue', lwd=2.5)\n#add vertical line for predicted release\nabline(v=rel/sr, col='red', lwd=2.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nNext up, by default we predict that the voicing onset is the first short interval where the mean autocorrelation of samples is above some baseline. We split the rest of the sound file after the predicted release into intervals with a duration of `vo_granularity` ms (default is `1`), and assume that the first voiced interval is the one where samples are on average autocorrelated more than `vo_param` % (default is `0.85`, i.e. 85%) of the most autocorrelated interval in the sound file. The general idea is that voiced intervals are periodic, so correlation between adjacent samples should generally be quite high. On the other hand, intervals that occur during a voiceless stop release are noisy, so correlation between adjacent samples should be very low.\n\nThis may need a bit of unpacking. The most autocorrelated 1 ms interval is indicated with a blue line here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set vo_granularity as default 1\nvo_granularity <- 1\n#convert vo_granularity to number of samples\nvo_int <- vo_granularity*step\n#make vector of intervals\nsq_vo <- seq(from=rel+(step*5), to=rel+(step*200), by=vo_int)\n\nmu_acf <- c()\ni <- 1\nfor (s in sq_vo) {\n  #run acf on samples in interval, don't plot, ignore NAs\n  acf <- stats::acf(sound[s:(s+step-1)], plot=F,\n                    na.action=stats::na.pass)\n  #fill vector with mean autocorrelation across all lags\n  mu_acf[i] <- mean(acf$acf)\n  i <- i+1\n}\n\nplot(y=sound, x=vl_ex$t, type='l', ylab='Amplitude', xlab='Time (s)')\n#add vertical line for most autocorrelated interval\nabline(v=sq_vo[which.max(mu_acf)]/sr, col='blue', lwd=2.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nIf we zoom into just that one ms interval, it looks like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#determine location of samples in most autocorrelated interval\nmax_acf <- ((which.max(mu_acf))*step)+rel\n#plot just those samples from the sound object\nplot(sound[max_acf:(max_acf+vo_int)], type='l', ylab='Amplitude', \n     xlab='Time (individual samples)')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nThis clearly looks periodic. If we use the function `acf()` as below, we can see that adjacent samples are highly correlated, and autocorrelation with samples at lags up to six samples removed is generally quite high.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#rerun acf on that interval, plot results this time\nacf(sound[max_acf:(max_acf+vo_int)], main='')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nThis, on the other hand, is our least autocorrelated interval:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(y=sound, x=vl_ex$t, type='l', ylab='Amplitude', xlab='Time (s)')\n#add vertical line for least autocorrelated interval\nabline(v=rel/sr+sq_vo[which.min(mu_acf)]/sr, col='blue', lwd=2.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\nIf we zoom in to just that one interval, we see a very jagged line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmin_acf <- ((which.min(mu_acf))*step)+rel\nplot(sound[min_acf:(min_acf+vo_int)], type='l', ylab='Amplitude', \n     xlab='Time (individual samples)')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\nAnd running `acf()` again, we see that autocorrelation is very low.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nacf(sound[min_acf:(min_acf+vo_int)], main='')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\nThe first interval where the mean autocorrelation is above 85% of the most autocorrelated interval (i.e., the predicted voicing onset) is indicated with a red line here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set vo_param as default 0.85\nvo_param <- 0.85\n\n#create vector of intervals where autocorrelation is above voicing onset baseline\nhi_acf <- which(mu_acf > max(mu_acf, na.rm=T)*vo_param)\n#voicing onset is predicted to be 1ms after the start of the second interval \n#above the baseline. helps to reduce the influence of random fluctuations, and\n#usually the start of the first interval above the baseline precedes visible voicing.\nvo <- (rel + (hi_acf[2]*(step*vo_granularity)) + step)\n\nplot(y=sound, x=vl_ex$t, type='l', ylab='Amplitude', xlab='Time (s)')\nabline(v=sq_vo[which.max(mu_acf)]/sr, col='blue', lwd=2.5)\nabline(v=vo/sr, col='red', lwd=2.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\nUsing the default parameters, this is exactly what goes on under the hood when the function `positiveVOT()` is called! One of the default parameters is `vo_method='acf'`, which implements the autocorrelation-based method for predicting voicing onset that we just saw. This is generally the most precise and by far the fastest method, but it is also a little fickle and will fail for lower quality audio or adverse recording conditions.\n\nFor this reason, another possibility is using the alternative `vo_method='f0'`. This method simply runs the pitch tracking algorithm implemented in the `pitchtrack()` function in the `phonTools` package, and predicts that voicing onset begins at the time of the first successful pitch measure after the stop release. The arguments `f0_wl` and `f0_minacf` are passed along to `phonTools::pitchtrack()` (as that function's arguments `minacf` and `windowlength`). Note that the default `f0_wl` is 30 ms, which is quite a bit lower than the `phonTools::pitchtrack()` default of `50`; this seems to give better results when the purpose isn't actually to measure pitch, but to find the first instance of voicing.\n\nFinally, `positiveVOT()` also takes the argument `f0_first`, where the default is `FALSE`. If set to `TRUE`, the prediction process starts by finding the longest stretch of continuous pitch using `phonTools::pitchtrack()`, and subsequently looking for a stop release only within the 200 ms preceding the beginning of that stretch. As with `vo_method='f0'`, this is a relatively slow method, but can give better results when the recordings conditions and/or audio quality is suboptimal.\n\n### Negative VOT\n\nNegative VOT is predicted using the function `negativeVOT()`. The process will be exemplified using the same voiced stop as we saw above. Just as when predicting positive VOT, the first step is to locate a stop closure by searching for the most silent interval of duration `closure_interval` (default is `10`) in the first half of the sound file. The implementation and result of this are shown here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#extract sound samples and overwrite sound object\n#see same procedure for positiveVOT() above for commented code\nsound <- vd_snd$sig[,1]\nclosure_interval <- 10\nci <- closure_interval/1000\nsqlen <- ceiling(length(sound)/2 / sr / ci)\nsq_clo <- seq(from=sr*ci, to=sqlen*sr*ci, by=sr*ci)\n\nmean_amp <- c()\ni <- 1\nfor (s in sq_clo) {\n  mean_amp[i] <- mean(abs(sound[(s-(sr*ci)+1):s]))\n  i <- i+1\n}\nclo <- (which(mean_amp==min(mean_amp)) - 0.5) * sr * ci\n\nplot(y=sound, x=vd_snd$t, type='l', ylab='Amplitude', xlab='Time (s)')\nabline(v=clo/sr, col='red', lwd=2.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\nThe default method for finding voicing onset is very similar for voiced and voiceless stops: the rest of the sound file after the predicted time of closure is divided into short intervals of duration `vo_granularity` (default is here `1.2` ms), and voicing onset is predicted to be the first interval where autocorrelation of samples within the interval is above some baseline -- default is 90% of the most highly autocorrelated interval in the sound file (`vo_param = 0.9`). The implementation and result of this are shown here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set vo_granularity as default 1.2\nvo_granularity <- 1.2\n#create vector of intervals\nsq_vo <- seq(from=clo, to=length(sound), by=(step*vo_granularity))\n#set vo_param as default 0.9\nvo_param <- 0.9\nmu_acf <- c()\ni <- 1\n#see same procedure for positiveVOT() above\nfor (s in sq_vo) {\n  acf <- stats::acf(sound[s:(s+(step*vo_granularity)-1)], plot=F,\n                    na.action=stats::na.pass)\n  mu_acf[i] <- mean(acf$acf)\n  i <- i+1\n}\n\nhi_acf <- which(mu_acf > max(mu_acf, na.rm=T)*vo_param)\nf0_start <- (clo + (hi_acf[2]*(step*vo_granularity)) + step)\n\nplot(y=sound, x=vd_snd$t, type='l', ylab='Amplitude', xlab='Time (s)')\nabline(v=f0_start/sr, col='red', lwd=2.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\nAs with `positiveVOT()`, `negativeVOT()` offers the alternative method for locating voicing onset `vo_method='f0'` with the default settings `f0_wl=50` and `f0_minacf=0.5` passed onto the `pitchtrack()` function of the `phonTools` package.\n\nFinding the release of a prevoiced stop is trickier. The default method of `negativeVOT()` is to search for the so-called *transient phase* (`rel_method='transient'`). The transient phase of a stop is when the compressed air in the oral tract is discharged immediately at the time of the stop release. This results in a fairly even distribution of energy throughout the spectrum with a linear drop-off in energy at higher frequencies -- in other words, a particularly *smooth* spectrum. These are tricky to find automatically as they're very short.\n\nThe way this is implemented in `negativeVOT()` is by using the `spectralslice()` function in `phonTools` to generate FFT spectral slices of 1 ms sound intervals at each 10 samples after the predicted voicing onset. An FFT spectrum in R is just a vector of power measures by frequency bins; I estimate spectral smoothness by taking the standard deviation of the difference in power among adjacent frequency bins in the spectrum. A low number should indicate a smooth spectrum. I'll unpack this with some code and plots.\n\nThis bit of code generates the spectra and calculates smoothness:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#to save time, search only half the duration of the sound file after predicted voicing onset\nsrch <- sound[f0_start:(length(sound)/2)]\n#set starting times each 10 samples for intervals to generate spectral slices from\nsteps <- 10:(round(length(srch)/step)*10)\nsmoothness <- c()\n\n#loop through starting times\nfor (i in steps) {\n  #determine 1 ms interval times\n  stp <- f0_start + ((i*step) / 10)\n  #generate FFT spectral slice, don't plot\n  #spectralslice() returns a matrix, the second column contains power estimates, save those\n  slice <- phonTools::spectralslice(sound[(stp-step+1):stp], show=F)[,2]\n  #fill vector with spectral smoothness estimates\n  smoothness[i-9] <- stats::sd(diff(slice))\n}\n```\n:::\n\n\nThis is an example of a rather jagged spectrum with a fairly high power difference between adjacent frequency bins. There's voicing here, and especially the difference between the first few frequency bins is huge.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#find most jagged spectrum (high \"smoothness\" estimate = low smoothness)\nmax_smooth <- which(smoothness == max(smoothness))\n#determine location of most jagged spectrum\njagged <- f0_start + ((steps[max_smooth]*step)/10)\n#generate that jagged spectrum again for visualization purposes, don't use \n#default plotting function because it doesn't return frequency in Hz scale\njagged_spec <- phonTools::spectralslice(sound[(jagged-step+1):jagged], show=F)\n#plot spectrum. first matrix column has frequency values, second has power estimates\nplot(x=jagged_spec[,1]*10000, y=jagged_spec[,2], type='l', \n     ylab='Power (dB)', xlab='Frequency (Hz)')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\nWe can demonstrate this by plotting just the difference in power among adjacent frequency bins, instead of plotting the actual spectrum:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#plot power difference between adjacent frequency bins. must exclude first bin on x axis,\n#because diff(x) returns a vector of size x-1\nplot(y=diff(jagged_spec[,2]), x=jagged_spec[,1][-1]*10000, type='l', \n     ylab='Diff. betwen adjacent measures (dB)', xlab='Frequency (Hz)')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\nBy comparison, this is the smoothest spectrum in the file:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmin_smooth <- which(smoothness == min(smoothness))\nsmooth <- f0_start + ((steps[min_smooth]*step)/10)\nsmooth_spec <- phonTools::spectralslice(sound[(smooth-step+1):smooth], show=F)\nplot(x=smooth_spec[,1]*10000, y=smooth_spec[,2], type='l', \n     ylab='Power (dB)', xlab='Frequency (Hz)')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\nThis is immediately clear when plotting the difference between adjacent frequency bins using the same range for the Y-axis as we did for the less smooth spectrum above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(y=diff(smooth_spec[,2]), x=smooth_spec[,1][-1]*10000, type='l', \n     ylab='Diff. betwen adjacent measures (dB)', xlab='Frequency (Hz)',\n     #set limits on y axis identically to the previous difference plot\n     ylim=range(diff(jagged_spec)))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\nAnd this is the location in the sound file of that very smooth spectrum shown in red:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#extract location of predicted release in sound file, actually just slightly after smoothest spectrum\npred_rel <- f0_start + (((min_smooth+9)*step) / 10)\nplot(y=sound, x=vd_snd$t, type='l', ylab='Amplitude', xlab='Time (s)')\n#add vertical line at location of predicted release\nabline(v=pred_rel/sr, col='red', lwd=2.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\nAs with using autocorrelation for locating voicing onset, using the transient phase to locate the release of a voiced stop is *fickle*; if the audio quality is good, I have found that it tends to give remarkably precise results. On the other hand, when it fails, it fails completely. It is also relatively slow, as it involves generating a lot of spectral slices. An alternative method is simply looking for increases in amplitude, as the amplitude of prevoicing is usually low. This method essentially looks for the beginning of the vowel rather than the stop release itself, and as such results are somewhat less precise. This method can be set with the parameter `rel_method='amplitude'`.\n\nHere, we note the maximum amplitude in 1 ms intervals after the predicted voicing onset. This results in a time series that looks like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create a vector of 500x 1ms intervals starting at predicted voicing onset\nsq_rel <- seq(from=f0_start, to=f0_start+(step*500), by=step)\n\nmax_amp <- c()\ni <- 1\nfor (s in sq_rel) {\n  #fill vector with highest amplitude of each interval\n  max_amp[i] <- max(sound[s:(s+(step)-1)])\n  i <- i+1\n}\n#remove any NAs from max_amp vector if 500x 1ms intervals are not available \n#(not really necessary here)\nif (length(which(is.na(max_amp))) > 0) {\n  max_amp <- max_amp[-which(is.na(max_amp))]\n}\n\n#plot maximum amplitude per interval as time series\nplot(max_amp, type='l', ylab='Max. amplitude in window', xlab='Window')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\nThis is much too jagged to be of any use, but we can get a clearer picture by smoothing it out. We do this using a discrete cosine transformation, as implemented in the `dct()` function of `emuR`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#smooth above time series using DCT based on 25 coefficients, don't plot, store fitted object\ndct_fit <- emuR::dct(max_amp, m=25, fit=T)\n#plot smoothed time series\nplot(dct_fit, type='l', ylab='Max. amplitude in window (smoothed)', xlab='Window')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\nThe predicted stop release is 5 ms before we see the highest velocity in this time series. The highest velocity is indicated with a blue line here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#save vector with difference between each adjacent max amplitude from smoothed time series\ndiff_amp <- diff(dct_fit)\n#replot smoothed time series\nplot(dct_fit, type='l', ylab='Max. amplitude in window (smoothed)', xlab='Window')\n#add vertical line with location of highest difference\nabline(v=which.max(diff_amp), col='blue', lwd=2.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\nThis predicts this stop release location:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#locate predicted stop release, 5ms prior to highest amplitude increase\npred_rel_amp <- sq_rel[which.max(diff_amp)] - (step * 5)\n\nplot(y=sound, x=vd_snd$t, type='l', ylab='Amplitude', xlab='Time (s)')\n#add vertical line for predicted stop release\nabline(v=pred_rel_amp/sr, col='red', lwd=2.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\nThat covers all the parameters of `negativeVOT()`.\n\n## Source of data\n\nThe sound files used for demonstrating the `getVOT` functions throughout the tutorial are in Kmhmu', taken from [this OSF repository](https://osf.io/wv6qz). The data were collected to study transphonologization of onset voicing. See @kirby2022.\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}